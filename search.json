[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/hw1/index.html",
    "href": "posts/hw1/index.html",
    "title": "HW 1 - Analyzing and Comparing Temps of Various Countries",
    "section": "",
    "text": "Step 1:\nimport all necessary packages\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport plotly.express as px\n\n\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\n\nStep 2\n\nImport the csv files: temps, countries, and stations\nThen, we will use the prepare_df function to reorganize the temps csv file. We can preview the temps file after we run it through the function to see the changes.\n\n\ntemps = pd.read_csv(\"temps.csv\")\n\ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries = pd.read_csv(countries_url)\n\nstations = pd.read_csv(\"station-metadata.csv\")\n\n\ndef prepare_df(df):\n    \"\"\"\n    Prepares the inputed dataframe by reorganzing the data.\n\n    Parameters:\n    - df: inputted dataframe containing temperature data.\n\n    Returns:\n    df: New dataframe with columns 'ID', 'Year', 'Month', and 'Temp'.\n                  The 'Month' column represents the month in integer form, and the 'Temp' column\n                  represents the temperature values\n    \"\"\"\n    \n    \n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\n\ntemps = prepare_df(temps)\ntemps\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\n\n\n\n\n0\nACW00011604\n1961\n1\n-0.89\n\n\n1\nACW00011604\n1961\n2\n2.36\n\n\n2\nACW00011604\n1961\n3\n4.72\n\n\n3\nACW00011604\n1961\n4\n7.73\n\n\n4\nACW00011604\n1961\n5\n11.28\n\n\n...\n...\n...\n...\n...\n\n\n13992657\nZIXLT622116\n1970\n8\n15.40\n\n\n13992658\nZIXLT622116\n1970\n9\n20.40\n\n\n13992659\nZIXLT622116\n1970\n10\n20.30\n\n\n13992660\nZIXLT622116\n1970\n11\n21.30\n\n\n13992661\nZIXLT622116\n1970\n12\n21.50\n\n\n\n\n13992662 rows × 4 columns\n\n\n\nWe can see that now that the Month column now reflects all months, instead of each month being in its own column\n\n\nStep 3:\nUsing SQLITE, create a data base called “temps.db”. We link the temps file to “temperatures” We link the stations file to “stations” We link the countries file to “countries”\nNow all three data sets live within our temps.db data base\nWe make sure to close our connection to the database once we created it.\n\nwith sqlite3.connect(\"temps.db\") as conn:\n    temps.to_sql(\"temperatures\", conn, if_exists = \"replace\", index = False)\n    stations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n    countries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n\nconn.close()\n#closes connection\n\n\n\nStep 4:\nImport the query_climate_database function. This function takes 5 parameters: db_file, country, year_begin, year_end, & month. - db_file is the name of our database file. In this case, its “temps.db” - country is the name of the country we are inquiring about. - year_begin is the beginning of the range of years we are inquiring about - year_end is the ending of the range of years we are inquiring about - month is the month number we are inquring about (example: January is 1)\n\nwe use SELECT to select the specified columns, JOIN to put the columns from multiple tables next to one another, and WHERE to specify conditions using the parameters\n\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Query a climate database to retrieve temperature data for a specific country, year range, and month.\n\n    Parameters:\n    - db_file: database file.\n    - country: name of the country for which climate data is requested.\n    - year_begin: starting year of the desired temperature data.\n    - year_end: ending year of the desired temperature data.\n    - month: month for which temperature data is requested.\n\n    Returns:\n    pd: dataframe containing weather station information ('NAME', 'LATITUDE', 'LONGITUDE', 'COUNTRY'),\n                  along with temperature data ('YEAR', 'MONTH', 'ID', 'TEMP')\n                  \n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    cursor = conn.cursor()\n    #cursor.execute(f'SELECT C.Name, SUBSTR(T.ID,1,2) FROM temperatures T LEFT JOIN countries C on T.\"FIPS 10-4\" = C.\"FIPS 10-4\" WHERE C.Name=\"{country}\"')\n    cursor.execute(f'SELECT C.Name, SUBSTR(T.ID,1,2) FROM temperatures T JOIN countries C WHERE C.Name=\"{country}\" AND SUBSTR(T.ID,1,2) = C.\"FIPS 10-4\"')  \n\n    result = [cursor.fetchone() for i in range(1)]  # get just the first  result\n    #this gives us Name and first two letters\n    \n    letters = result[0][1] # this gives us IN\n    \n    x = cursor.execute(f'SELECT T.Year, T.Temp, T.ID FROM temperatures T WHERE SUBSTR(T.ID,1,2)=\"{letters}\" AND T.Year &lt;= {year_end} AND T.Year &gt;= {year_begin} AND T.Month={month}')\n    # we now have year and temp for the given inputs\n    #still need name of weatherpeerson and latitude/longitude\n\n    cursor = conn.cursor()\n    y=cursor.execute(f'SELECT S.NAME, S.LATITUDE, S.LONGITUDE, S.ID FROM stations S LEFT JOIN temperatures T on T.ID = S.ID WHERE SUBSTR(S.ID,1,2)=\"{letters}\"')\n    \n    \n    \n\n    # putting all info into data frame\n    df = pd.DataFrame(x, columns = [\"YEAR\", \"TEMP\", \"ID\"])\n    df2 = pd.DataFrame(y, columns = [\"NAME\", \"LATITUDE\", \"LONGITUDE\", \"ID\"])\n    df2[\"COUNTRY\"] = country\n    #print(cursor.fetchall())\n    merged = pd.merge(df, df2, on='ID')\n    dropped = merged.dropna()\n    final = dropped.drop_duplicates()\n    \n    \n\n    # Reorder columns\n    reordered = final[[\"NAME\", \"LATITUDE\", \"LONGITUDE\", \"COUNTRY\", \"YEAR\", \"ID\", \"TEMP\"]]\n    \n    #changing the ID column to display the month instead\n    reordered[\"ID\"] = month\n    final2 = reordered.rename(columns={'ID': 'MONTH'})\n    \n    #this resets the indexes\n    reset = final2.reset_index(drop=True)\n\n    reset2 = reset.rename(columns={'COUNTRY': 'Country', 'YEAR': 'Year', 'MONTH': 'Month', 'TEMP': 'Temp'})\n\n\n    conn.close()\n    return reset2\n\n\n\n\nnow test our function\n\n\ntest = query_climate_database(\"temps.db\", \"India\", 1980, 2020, 1)\ntest\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\nOur function worked! It gave us the temepratures for all stations in India. The temperatures are from January, measured between 1980 and 202\n\n\nStep 5:\nNow that we can easily pull data using our function, we can make interactive plots of our data! We will use plotly to create a geographic graph of the weather stations.\nWe are going to answer this question with our plot: How does the average yearly change in temperature vary within a given country?\nTo do this, we will write a function called temperature_coefficient_plot that takes 6 parameters: db_file, country, year_begin, year_end, month, min_obs, **kwargs - db_file is the name of our database file. In this case, its “temps.db” - country is the name of the country we are inquiring about. - year_begin is the beginning of the range of years we are inquiring about - year_end is the ending of the range of years we are inquiring about - month is the month number we are inquring about (example: January is 1) - min_obs is the minimum number of observations a station needs to have reported to be included in our graph\nTo measure the average yearly change, we will have to use linear regression to create a model and find the coefficient representing the estimated yearly increase. We will then have to put this into a data frame, find each stations latitude and longitude, and plot it geographically.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \n    def estimate_yearly_increase(df):\n        \"\"\"\n    Estimates the yearly increase in temperature based on a linear regression model.\n\n    Parameters:\n    - df: inputted dataframae containing temperature data with columns 'YEAR' and 'TEMP'.\n\n    Returns:\n    estimated_yearly_increase: Estimated yearly increase in temperature. This is the coefficient of a linear regression model.\n    The linear regression model is fitted using the 'YEAR' column as the independent variable and the 'TEMP'\n    column as the dependent variable.\n    \"\"\"\n        \n        \n        \n        X = df[[\"YEAR\"]]\n        y = df[\"TEMP\"]\n\n        # Create a linear regression model\n        model = LinearRegression()\n\n        # Fit the model\n        model.fit(X, y)\n\n        # Get the coefficient representing the estimated yearly increase\n        estimated_yearly_increase = model.coef_[0]\n\n        return estimated_yearly_increase\n\n    #uses above funciton to create data frame\n    test = query_climate_database(db_file, country, year_begin, year_end, month)\n\n    #groups all the stations w same lattide and longitude together, then applies the regression\n    result = test.groupby(['LATITUDE', 'LONGITUDE']).apply(estimate_yearly_increase).reset_index()\n\n    #this gives us how many rows relate to each station\n    groups = test.groupby(['LATITUDE', 'LONGITUDE']).transform('size')\n\n    #adds a size column\n    result[\"size\"] = groups\n\n    # filter out rows with a size less than min_obs\n    result = result[result['size'] &gt;= min_obs]\n\n    # merge the original data frame with the one that underwent linear regression\n    merged = pd.merge(result, test, on=['LATITUDE', 'LONGITUDE'])\n\n    \n    # rename columns\n    merged.columns = [\"LATITUDE\", \"LONGITUDE\", \"Estimated Yearly Increase (C°)\", \"SIZE\",\n                      \"NAME\", \"COUNTRY\", \"YEAR\", \"MONTH\", \"TEMP\"]\n\n    # round estimated increase\n    merged['Estimated Yearly Increase (C°)'] = merged['Estimated Yearly Increase (C°)'].round(4)\n\n    # create a scatter mapbox plot using plotly \n    fig = px.scatter_mapbox(merged,\n                            title='Estimated Yearly Increase in Temperature',\n                            lat='LATITUDE',\n                            lon='LONGITUDE',\n                            color='Estimated Yearly Increase (C°)',\n                            hover_name='NAME',\n                            mapbox_style=\"carto-positron\")\n\n    return fig\n\nNow test our function\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\nfig = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig\n\n\n\n\nNow we have a geographic graph of India that shows the estiamted yearly increase in temperature for each station. This pulls data from 1980 to 2020 in the month of January\nLets do this one more time for Mexico to ensure our code works correctly. This time we will set the years to be between 1999 and 2020 and for the month of February.\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\nfig2 = temperature_coefficient_plot(\"temps.db\", \"Mexico\", 1999, 2020, 2, \n                                   min_obs = 2,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig2\n#fig.show()\n\n\n\n\n\n\nStep 6:\nNow we will make a function to plot a box plot of temperature throughout the entire country for a given month and year.\nOur function will be called temp_for_year and take 4 arguments - db is the name of our database file. In this case, its “temps.db” - country is the name of the country we are inquiring about. - month is the month number we are inquring about (example: January is 1)\nIt will use the query_climate_database to pull data for the specific country, for the given year, and for the given month. This isn’t a range of years, rather data from a specific year.\nFor our boxplot, we need an x axis, which would be the month The y axis is the temp\n\n# box plot of temperature throughout entire country for a given month and year\n\ndef temp_for_year(db, country, year, month):\n    \"\"\"\n    Generate a box plot visualizing temperature distribution in a specific country for a given year and month.\n\n    Parameters:\n    - db: SQLite database file containing climate data.\n    - country: name of the country for which temperature data is visualized.\n    - year: year for which temperature data is visualized.\n    - month: month for which temperature data is visualized.\n\n    Returns:\n    fig: box plot displaying the distribution of temperatures for the specified country,\n                                  year, and month. The plot is created using Plotly Express.\n\n    \"\"\"\n    \n    \n    table = query_climate_database(db_file = db,\n                       country = country, \n                       year_begin = year,\n                       year_end = year,\n                       month = month)\n\n\n\n    fig = px.box(table,\n             title = f\"Temperature in {country} for {year}\",\n             x = \"MONTH\",\n             y = \"TEMP\",\n             color = \"MONTH\",\n             width = 600,\n             height = 600)\n\n    return fig\n\nNow lets test our function\n\ntemp_for_year(\"temps.db\", \"India\", 2020, 5)\n\n\n\n\nThis shows us that the median temperature in India for May of 2020 was 31.09 degreees celcius.\n\n\nStep 7:\nCreate a graph to visualize the average temperature in the entire country throughout the year, and to compare it to another country.\nTo do this, we will define a function called avg_temp_per_month with 4 parameters: file, country1, country2, year\n\nfile is the name of our database file. In this case, its “temps.db”\ncountry1 is the name of the first country we are inquiring about\ncountry2 is the name of the second country we are inquiring about\nyear is the year we are inquring about\n\ncomments throughout the function definition explain the code\nWe will also import a new query function, called query_climate_database2. This takes 3 parameters: file, country, and year. The function outputs a data frame with the average temperatues across the given country for each calander month of the year.\nFirst, we will import query_climate_database2 and test it. Then we will define the function avg_temp_per_month that makes calls to the query_climate_database2 function.\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database2(db_file, country, year):\n    \"\"\"\n    A new query a climate database to retrieve temperature data for a specific country, year range, and month.\n\n    Parameters:\n    - db_file: database file.\n    - country: name of the country for which climate data is requested.\n    - year: yearfor which temperature data is requested.\n\n    Returns:\n    pd: dataframe containing the monthly average temperature for the specified country and year.\n                  Columns include 'Month', 'ID', 'Average_Temp', and 'Country'.\n                  \n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    cursor = conn.cursor()\n    cursor.execute(f'SELECT C.Name, SUBSTR(T.ID,1,2) FROM temperatures T JOIN countries C WHERE C.Name=\"{country}\" AND SUBSTR(T.ID,1,2) = C.\"FIPS 10-4\"')  \n\n    result = [cursor.fetchone() for i in range(1)]  # get just the first  result\n    #this gives us Name and first two letters\n    \n    letters = result[0][1] # this gives us IN\n    \n    x = cursor.execute(f'SELECT T.Year, T.Temp, T.ID, T.Month FROM temperatures T WHERE SUBSTR(T.ID,1,2)=\"{letters}\" AND T.Year == {year}')\n    # we now have year and temp for the given inputs\n    #still need name of weatherpeerson and latitude/longitude\n\n    cursor = conn.cursor()\n    y=cursor.execute(f'SELECT S.ID FROM stations S LEFT JOIN temperatures T on T.ID = S.ID WHERE SUBSTR(S.ID,1,2)=\"{letters}\"')\n    \n    \n    \n\n    # putting all info into data frame\n    df = pd.DataFrame(x, columns = [\"YEAR\", \"TEMP\", \"ID\", \"Month\"])\n    df2 = pd.DataFrame(y, columns = [\"ID\"])\n    df2[\"COUNTRY\"] = country\n    #print(cursor.fetchall())\n    merged = pd.merge(df, df2, on='ID')\n    dropped = merged.dropna()\n    final = dropped.drop_duplicates()\n    \n    \n    mean_temps = final.groupby(['ID', 'Month', 'COUNTRY'])['TEMP'].mean()\n\n    # reset index to turn the result into a DataFrame\n    mean_temps_df = mean_temps.reset_index()\n\n    # group by 'Month' and calculate the mean temperature for each month\n    monthly_mean_temps = mean_temps_df.groupby('Month')['TEMP'].mean()\n\n    # creates a new DataFrame with 'Month', 'ID', and 'Average_Temp' columns\n    result_df = pd.DataFrame(monthly_mean_temps.reset_index())\n    result_df['Country'] = country\n    \n    #rounds the temps temps to 4 decimal places\n    result_df['TEMP'] = result_df['TEMP'].round(4)\n\n    # adds each months name in the month column\n    result_df['Month'] = [\"Jan\", \"Feb\", \"Mar\", \"April\", \"May\", \"June\", \"July\", \"Aug\", 'Sept', \"Oct\", \"Nov\", \"Dec\"]\n\n\n    #return the result!\n    return result_df\n\n\n\n\n# testing our new query function - it will give us the average temperature across India for each month in 1980\ntest = query_climate_database2(\"temps.db\", \"India\", 1980)\ntest\n\n\n\n\n\n\n\n\nMonth\nTEMP\nCountry\n\n\n\n\n0\nJan\n19.9226\nIndia\n\n\n1\nFeb\n22.5644\nIndia\n\n\n2\nMar\n25.4641\nIndia\n\n\n3\nApril\n29.1662\nIndia\n\n\n4\nMay\n30.7662\nIndia\n\n\n5\nJune\n28.4487\nIndia\n\n\n6\nJuly\n27.0070\nIndia\n\n\n7\nAug\n26.7079\nIndia\n\n\n8\nSept\n27.1199\nIndia\n\n\n9\nOct\n26.2906\nIndia\n\n\n10\nNov\n23.1583\nIndia\n\n\n11\nDec\n20.2473\nIndia\n\n\n\n\n\n\n\nAs we can see, the average temperature for each month in India for 1980 was calcuated.\nNow its time to write our new graphing function\n\n# now we can add our new graphing function\ndef avg_temp_per_month(file, country1, country2, year):\n    \"\"\"\n    Generate a scatter plot comparing the average temperature in each month for two countries in a specific year.\n\n    Parameters:\n    - file:  SQLite database file containing climate data.\n    - country1: Name of the first country for temperature data comparison.\n    - country2: Name of the second country for temperature data comparison.\n    - year: Year for which average temperature data is compared.\n\n    Returns:\n    fig: Scatter plot displaying the average temperature in each month for two countries.\n                                 The plot is created using Plotly Express (px.scatter).\n    \"\"\"\n    \n    \n    \n    \n    #uses query function for country 1\n    c1 = query_climate_database2(db_file = file,\n                       country = country1, \n                       year= year)\n    \n    \n    # uses query function for country 2\n    c2 = query_climate_database2(db_file = file,\n                       country = country2, \n                       year= year)\n    \n    \n    # places the data frame of country 1 ontop of the data frame of country 2 to make one long data frame\n    stacked = pd.concat([c1, c2], ignore_index=True)\n\n    \n    # graphs the data frame\n    fig = px.scatter(data_frame = stacked,\n                 title=f\"Average Temp in each month for {country1} vs {country2} \",\n                 x = \"Month\",\n                 y = \"TEMP\",\n                 color = \"TEMP\",\n                 hover_name = \"TEMP\",\n                 hover_data = [\"Month\", \"TEMP\"],\n                 #size = \"Body Mass (g)\",\n                 #size_max = 8,\n                 width = 800,\n                 height = 500,\n                 #opacity = 0.5\n                 facet_col = \"Country\"\n                )\n\n    \n    return fig\n\n\n# now we can use the avg\navg_temp_per_month(\"temps.db\",\"India\",\"China\", 2020)\n\n\n\n\nAs we can see, the graphs display a side by side comparison of the average temps of 2 countries per month."
  },
  {
    "objectID": "posts/hw 3/index.html",
    "href": "posts/hw 3/index.html",
    "title": "HW 3 A Fun Message Bank!",
    "section": "",
    "text": "We are going to create a simple message bank using flask! This message bank will let users submit a message with their name, and view other’s messages!\n\nStep 1: Install Packages\n\nfrom flask import Flask, render_template, request, g\nfrom flask import redirect, url_for, abort\nimport sqlite3\n\napp = Flask(__name__)\n\nWe will be using flask to build our webpage. Additionally, we will use sqlite3 to create a database to house the messages. Finally, we will name our app.\n\n\nStep 2: Create a database function to house the messages\n\ndef get_message_db():\n    \"\"\"\n    Attempts to return the existing message database connection.\n\n    If the message database connection (g.message_db) already exists, the function returns it.\n    If not, it creates a new message database connection and initializes a 'messages' table\n    with 'handle' and 'message' columns inside the database.\n\n    Returns:\n        g.message_db: The message database.\n    \"\"\"\n\n    \n    \n    \n    try:\n     # attempt to return the existing message database connection from the global context (g)\n     # checks to see if the message_db exists, if it doesnt't it will create one using the except:\n\n        return g.message_db\n    except: \n        # creates the database\n        g.message_db = sqlite3.connect(\"message_db.sqlite\")\n        \n        # creates a table messages inside the data base with a handle and message column\n        # IF NOT EXISTS means this command will only be ran if the messages table doesnt exist\n        cmd = 'CREATE TABLE IF NOT EXISTS messages (handle TEXT, message TEXT)'\n        \n        # connects to database\n        cursor = g.message_db.cursor()\n        cursor.execute(cmd)\n        return g.message_db\n\nWe are defining the get_message_db function. This function will check to see if a database already exists, and if it doesn’t, we will create one. This is the try and except part. If the databse exists, it will just return the database. If it doesn’t, we will create the databse using sqlite. Addtionally, we will use the command CREATE TABLE IF NOT EXISTS to add a messages table to the database. This table will have a handle column and a message column. The handle will be used for the names of the messengers, and the message column will be for the messages from the users.\n\n\nStep 3: Create a database function to house the messages\n\ndef insert_message(request):\n    # Extract the message and handle from the request\n    \"\"\"\n    Inserts a user-submitted message into the 'messages' table of the message database.\n\n    parameters:\n        request: The Flask request object containing form data.\n\n    Returns:\n        message, name: A tuple containing the submitted message and the user's name/handle.\n    \"\"\"\n\n    \n    \n    \n    message = request.form.get(\"message\")\n    name = request.form.get(\"name\")\n    \n    \n    # gets database\n    database = get_message_db()\n    \n    # connects to database\n    conn = sqlite3.connect('message_db.sqlite')\n    \n    \n    cursor = conn.cursor()\n   \n    # inserts the message and handle into the messages table\n\n       # creates a function to insert the values into the table\n        # the ? represents where the paremeters of name, messages will be passed too\n    insert = \"\"\"\n    INSERT INTO messages (handle, message) VALUES (?, ?);\n    \"\"\"\n    cursor.execute(insert, (name, message))\n    \n     # commits the changes and close the connection\n    conn.commit()\n    conn.close()\n        \n        # returns the message and name that was submitted\n    return message, name\n\nThe insert_message function inserts user-submitted messages into the database. It takes the message and name variables from a Flask request, connects to the ‘message_db.sqlite’ database, and uses a cursor to run an INSERT command. The function then commits the changes and closes the database connection. The returned variables contain the the submitted message and user’s name.\n\n\nStep 4: Create a function to generate random messages from the database\n\n# generates n random mesages\ndef random_messages(n):\n    \"\"\"\n    Generates a list of n random messages taken from the 'messages' table in the message database.\n\n    parameters:\n        n: The number of random messages to generate.\n\n    Returns:\n        list: A list of messages, each containing the handle and message of a randomly selected entry.\n    \"\"\"\n    \n    # connects to the database\n    conn = sqlite3.connect(\"message_db.sqlite\")\n    cursor = conn.cursor()\n\n    # this gets us the total number of messages from the data base\n    # this way if n is less than the total number of messages, we won't run into any issues\n    cursor.execute(\"SELECT COUNT(*) FROM messages\")\n    total = cursor.fetchone()[0]\n\n    # determines the number of messages to retrieve (minimum of n and total_messages)\n    minimum = min(n, total)\n\n    # retrieve n random messages from the database\n    cursor.execute(f\"SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT {minimum}\")\n    finalmessages = cursor.fetchall()\n\n    # close the cursor and connection\n   \n    conn.close()\n\n    # returns the gathered messages\n    return finalmessages\n\nThe random_messages function outputs a list of n random messages from the ‘messages’ table in the database. It determines the total number of available messages and retrieves a minimum of n random entries. The resulting list consists of messages, each containing the handle and message.\n\n\nStep 5: Create the decorators.\nDecorators in are functions that use URLs to output specific things. Using the “@” symbol, decorators specifify what will come after the .com in a URL. Then, it uses a function to run code for when the URL is at the specified point.\n\n# www.google.com/\n@app.route(\"/\") # decorators\ndef render_base():\n    \"\"\"\n    Renders the main page, 'base.html', which serves as the base template for all other pages on the website.\n    'base.html' provides the header visible across the entire website.\n\n    Returns:\n        render_template(base.html): The HTML template for the base page.\n    \"\"\"\n    \n    \n    # renders the main page, base.html, which all other pages are built off of\n    # base.html gives us the header visible at all times while on the website\n    return render_template(\"base.html\")\n\n\n# route for the submit page\n@app.route(\"/submit/\", methods=['POST', 'GET'])\ndef submit():\n    \"\"\"\n    Renders the 'submit.html' template for a URL ending in /submit/.\n    \n    If the method is 'GET', (a user visit to the URL), the function returns the 'submit.html' template.\n    \n    If the method is 'POST', (a form submission), the function inserts the submitted message into the database\n    using the 'insert_message' function. It then renders the 'submit.html' template.\n    Returns:\n        render_template(submit.html): The HTML template based on the request method.\n    \"\"\"\n    if request.method == 'GET':\n         # if the user just visits the url\n        return render_template('submit.html')\n    else:\n        # inserts message into data base\n        message, name = insert_message(request)\n        \n        # render the submit template, thanking the person for their messaage\n        return render_template(\"submit.html\",message = message, name = name )  \n\n\n@app.route(\"/view/\")\ndef view():\n    \"\"\"\n    Renders the 'view.html' template for a URL ending in /view/.\n    \n    If there are messages present, it shows 5 random messages taken from the 'random_messages' function.\n    The rendered template includes the retrieved messages.\n\n    If no messages are present, it renders an empty 'view.html' template.\n\n    Returns:\n        render_template(view.html): The HTML template for the view page.\n    \"\"\"\n    try:\n        # if there are messages present, it will show 5 random messages\n        msg = random_messages(5)\n        return render_template(\"view.html\", messages = msg) \n    except:\n        # if no messages are present, it will render and empty view page\n         return render_template(\"view.html\") \n\n\n # route of main page\n@app.route(\"/main/\")\ndef render_main():\n    \"\"\"\n    Renders the 'main.html' template, the main page of the website.\n\n    Returns:\n        render_template(main.html): The HTML template for the main page.\n    \"\"\"\n    return render_template(\"main.html\")\n\n\n\nStep 6: Reviewing submit.html\n\n\n\nimage.png\n\n\nSubmit.html starts from extending from base.html. Base.html serves as a header for submit.html, with links to submit a mssage and view a message. base.html also includes the title of the wbesite.\nThe block header specifics the color of the text and the font (black and cursive). The header says “Put down your thoughts here!”\nThe block content is where a user can submit their message. It’s in the form “post”. It first asks for “Your message here” and has an input spot. The input takes in text as a “message”. The next text box is for “Name/Handle”. Under this is another input spot. This input spot also takes in text as a “name”. Then we have another input of tupe “submit”. This is just a submit button labeled with “Submit Message!” That ends the form.\nNow we have an if statement. If a name is successfully entered, after submitting their message the user will see a message that says “Thanks (their name here) for the message! To submit another message, hit”Submit a message” above!\nIf there is an error, it will output a text saying “Please resubmit”.\nThe final endblock ends our html code\n\n\nStep 7: Example Screenshots\n\n\n\nimage.png\n\n\nThe above image is an example of a user submitting a message.\n\n\n\nimage.png\n\n\nThe above image is what the user sees after submitting a message.\n\n\n\nimage.png\n\n\nThe above image is what the user sees when viewing messages\n\n\nStep 8: Link to github respository\nhttps://github.com/nicosm77/HW-3---Message-Page"
  },
  {
    "objectID": "posts/posts/index.html",
    "href": "posts/posts/index.html",
    "title": "HW 0",
    "section": "",
    "text": "Import all necessary packages, and import the Palmer Penguins data set. For this assignment, we use the pandas package and the matplotlib.pyplot package.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\ndf = pd.read_csv(url)"
  },
  {
    "objectID": "posts/posts/index.html#step-1-importing",
    "href": "posts/posts/index.html#step-1-importing",
    "title": "HW 0",
    "section": "",
    "text": "Import all necessary packages, and import the Palmer Penguins data set. For this assignment, we use the pandas package and the matplotlib.pyplot package.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\ndf = pd.read_csv(url)"
  },
  {
    "objectID": "posts/posts/index.html#step-2-previewing-data",
    "href": "posts/posts/index.html#step-2-previewing-data",
    "title": "HW 0",
    "section": "Step 2: Previewing Data",
    "text": "Step 2: Previewing Data\nPreview the data frame to see the column names and better understand what we’re working with.\n\ndf\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\nPAL0910\n120\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A2\nNo\n12/1/09\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n340\nPAL0910\n121\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN39A1\nYes\n11/22/09\n46.8\n14.3\n215.0\n4850.0\nFEMALE\n8.41151\n-26.13832\nNaN\n\n\n341\nPAL0910\n122\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN39A2\nYes\n11/22/09\n50.4\n15.7\n222.0\n5750.0\nMALE\n8.30166\n-26.04117\nNaN\n\n\n342\nPAL0910\n123\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN43A1\nYes\n11/22/09\n45.2\n14.8\n212.0\n5200.0\nFEMALE\n8.24246\n-26.11969\nNaN\n\n\n343\nPAL0910\n124\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN43A2\nYes\n11/22/09\n49.9\n16.1\n213.0\n5400.0\nMALE\n8.36390\n-26.15531\nNaN\n\n\n\n\n344 rows × 17 columns"
  },
  {
    "objectID": "posts/posts/index.html#step-3-breaking-up-the-data",
    "href": "posts/posts/index.html#step-3-breaking-up-the-data",
    "title": "HW 0",
    "section": "Step 3: Breaking up the data",
    "text": "Step 3: Breaking up the data\nWe see in the data there are three types of penguins: Adelie, Gentoo, and Chinstrap. We also see there’s data on each penguins flipper length and body mass.\nWe can build a scatterplot to see the relationship between the penguin’s flipper lenght and body mass. We can take this one step further and split this scatterplot up into the three types of penguins.\nLet’s start by breaking up the data frame into three smaller data frames. Each data frame will take all the entries pretaining to an individual species of penguin. Therefore, we will have 3 new data frames (since there are three species of penguins).\n\n# breaking up the data to be with certain species\nadelie = df[df[\"Species\"] == \"Adelie Penguin (Pygoscelis adeliae)\"]\ngentoo = df[df[\"Species\"] == \"Gentoo penguin (Pygoscelis papua)\"]\nchinstrap = df[df[\"Species\"] == \"Chinstrap penguin (Pygoscelis antarctica)\"]\n\nNow, we can preview the new data frame pretaining to the Adelie species.\n\nadelie\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n147\nPAL0910\n148\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN83A2\nYes\n11/13/09\n36.6\n18.4\n184.0\n3475.0\nFEMALE\n8.68744\n-25.83060\nNaN\n\n\n148\nPAL0910\n149\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN84A1\nYes\n11/17/09\n36.0\n17.8\n195.0\n3450.0\nFEMALE\n8.94332\n-25.79189\nNaN\n\n\n149\nPAL0910\n150\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN84A2\nYes\n11/17/09\n37.8\n18.1\n193.0\n3750.0\nMALE\n8.97533\n-26.03495\nNaN\n\n\n150\nPAL0910\n151\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN85A1\nYes\n11/17/09\n36.0\n17.1\n187.0\n3700.0\nFEMALE\n8.93465\n-26.07081\nNaN\n\n\n151\nPAL0910\n152\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN85A2\nYes\n11/17/09\n41.5\n18.5\n201.0\n4000.0\nMALE\n8.89640\n-26.06967\nNaN\n\n\n\n\n152 rows × 17 columns"
  },
  {
    "objectID": "posts/posts/index.html#step-4-building-the-scatterplot",
    "href": "posts/posts/index.html#step-4-building-the-scatterplot",
    "title": "HW 0",
    "section": "Step 4: Building the Scatterplot",
    "text": "Step 4: Building the Scatterplot\nNow that we have our individual data frames, we can build our scatterplot using the plt.scatter command. We will index the data within the columns “Body Mass (g)” and “Flipper Length (mm)” for our x and y axis. We will color code each speciesof penguin as well.\nWe can also add x and y axis labels, a plot title, and a legend.\n\nplt.scatter(adelie['Body Mass (g)'], adelie['Flipper Length (mm)'], marker='o', linestyle='', color='b', label='Adelie Penguin')\nplt.scatter(gentoo['Body Mass (g)'], gentoo['Flipper Length (mm)'], marker='o', linestyle='', color='red', label='Gentoo Penguin')\nplt.scatter(chinstrap['Body Mass (g)'], chinstrap['Flipper Length (mm)'], marker='o', linestyle='', color='green', label='Chinstrap Penguin')\n\n\nplt.xlabel('Body Mass (grams)')\nplt.ylabel('Flipper Length (mm)')\nplt.title('Plotting Flipper Length vs Body Mass')\n\n\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\nModuleNotFoundError: No module named 'climate_database'"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/hw2/index.html",
    "href": "posts/hw2/index.html",
    "title": "HW 2 Scrapy Movies Recomendations",
    "section": "",
    "text": "This code is a web crawling and scraping framework in Python. It uses a spider to scrape. The spider navigates through movie pages on themoviedb.org, extracts information about actors, and retrieves details about their movies or TV shows.\n\nStep 1: Import packages\n\nimport scrapy\n\n\n\nStep 2: Writing the Spider Class:\n\nclass TmdbSpider(scrapy.Spider):\n    \n    # name of spider\n    name = 'tmdb_spider'\n\nWe create the TmdbSpider class usign scrapy.Spider, and name our webscraper ‘tmdb_spider’\n\n\nStep 3: Constructor and Initialization:\n\n# constructor - starts spider with the starting URL\ndef __init__(self, subdir=None, *args, **kwargs):\n    self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nThe constructor starts the spider with a starting URL based on the provided subdir. The subdir is the back half of the link for the specific movie we are inquring about.\n\n\nStep 4: First parsing method\n\n# assuming we are starting on the movie page, and then navigating to the Full Cast & Crew Page\ndef parse(self, response):\n    \"\"\"\n    Parses TMDB movie pages.\n    Assumes that you start on a movie page, and then navigates to the Full Cast & Crew page.\n    Yields a scrapy.Request for the \"Full Cast & Crew\" page, with the parse_full_credit method specified in the callback argument.\n    Does not return any data.\n    \"\"\"\n    # full cast and crew page has url: &lt;movie_url&gt;cast\n    full_credits_url = response.url + \"/cast/\"\n      \n    # yielding a scrapy request using full_credits_url and callback\n    # callback means calling parse_full_credits(self, responce) using the full_credits_url ( so we have navigated -\n    # to the full cast and crew page\n    yield scrapy.Request(full_credits_url, callback=self.parse_full_credits)\n\nIn the parse method, the spider navigates from the movie page to the Full Cast & Crew page by adding “/cast/” to the current URL. A scrapy request is yielded with the URL of the Full Cast & Crew page and the callback function parse_full_credits.\n\n\nStep 5: Second Parsing Method\n\n    # parse_full_credits to get the actor URLs from the Full Cast & Crew page\ndef parse_full_credits(self, response):\n     \"\"\"\n     Parses TMDB \"Full Cast & Crew\" pages.\n     Assumes that you start on the Full Cast & Crew page\n     Yields a scrapy.Request for the page of each cast member, with the parse_actor_page method specified in the callback argument.\n     Does not return any data.\n     \"\"\"\n   \n    # this gets each actor URL from the full cast and crew page using CSS selector   \n    #gets full info for all actors\n     actors = response.css('div.info')\n        \n     # extracts indidual urls of each actor\n     actors = actors.css('div.info &gt; p &gt; a::attr(href)').getall()\n        \n\n       # iterates thru each actor URL and yields a request for the actor url and the parse_actor_page as the callback\n     for urlx in actors:\n        #yield {\"actor\" :\"https://www.themoviedb.org\" + urlx}\n        yield scrapy.Request(\"https://www.themoviedb.org\" + urlx, callback=self.parse_actor_page)\n\nThe parse_full_credits method extracts actor URLs from the Full Cast & Crew page using CSS selectors. For each actor URL, a scrapy request is yielded with the actor URL and the callback function parse_actor_page.\n\n\nStep 6: Final Parsing Method\n\n# parse_actor_page to get actor name and their movies/tv shows from the actor's page\ndef parse_actor_page(self, response):\n    \"\"\"\n    Parses TMDB actor pages.\n    Assumes we are on each actor's individual page\n    Yields a dictionary containing the actor's name(\"actor\") and the movie/TV (\"movie_or_TV_name\") for each movie/TV show they have acted in.\n    This dictionary does not include any movie/TV they were a cast in.\n    \"\"\"\n     \n    # get the actor's name from the actor's page\n    name = response.css('div.title &gt; h2 &gt; a::text').get()\n\n\n        \n    # looks for acting header, saves it to index\n    index = response.css('div.credits_list h3::text').getall().index(\"Acting\")\n        \n    # looks for acting roles (exluding all other roles), so it uses the index of the acting header\n    roles = response.css('div.credits_list table.card.credits')[index]\n        \n    # gets name of the roles\n    final = roles.css('a.tooltip bdi::text').getall()\n        \n    # iterates through each move/tv show and gets a dictionary with actor and movie/TV show names\n    for movie_or_TV_name in final:\n            yield {\"actor\": name, \"movie_or_TV_name\": movie_or_TV_name}\n                \n\nThe parse_actor_page method extracts the actor’s name from the actor’s page. It finds the index of the “Acting” header and extracts the roles (movies/TV shows). It only takes the movies/TV shows the actor starred in, and leaves out any set they worked on as a crew or production.\nA dictionary with actor name and movie/TV show details is yielded for each entry. This dictionary displays all the movies/tv shows each actor in the movie additionally worked on\nThe spider, when run with the command scrapy crawl tmdb_spider -o movies.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone, will start from the specified movie page on themoviedb.org, navigate to the Full Cast & Crew page, and collect information about actors and their associated movies or TV shows. The output will be saved to a CSV file named movies.csv.\n\n\nStep 7: Analyzing Data\nNow that the date is in a movies.csv file, we can read the data using pandas data frame\n\n# reading in our data\nimport pandas as pd\ndf = pd.read_csv(\"results.csv\")\n\n\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nDaniel Radcliffe\nHave I Got a Bit More News for You\n\n\n1\nDaniel Radcliffe\nDavid Holmes: The Boy Who Lived\n\n\n2\nDaniel Radcliffe\n100 Years of Warner Bros.\n\n\n3\nDaniel Radcliffe\nMulligan\n\n\n4\nDaniel Radcliffe\nDigman!\n\n\n...\n...\n...\n\n\n2952\nRupert Grint\nThe View\n\n\n2953\nRupert Grint\nGMTV\n\n\n2954\nRupert Grint\nThe Tonight Show with Jay Leno\n\n\n2955\nRupert Grint\nAn Audience with...\n\n\n2956\nRupert Grint\nToday\n\n\n\n\n2957 rows × 2 columns\n\n\n\nNow we can see our data frame has a column for each actor, and another column with the movie or tv show name that they’ve starred in. We can clean up this data frame a bit.\n\n# need column of movies and a column of number of shared actors for that movie\n\n# value_counts() counts the number of times each movie or tv name appears in the column\nnumber = df[\"movie_or_TV_name\"].value_counts()\n# we can see now that Harry Potter and the Philosopher's Stone appears 63 times, \n# and Harry Potter and the Chamber of Secrets appears 37 times, etc.\n\n# puts into a new dataframe\nfinal = number.to_frame()\nfinal = final.reset_index()\n\n# renames column\nfinal.rename(columns={\"count\": \"Number of shared actors\"},inplace=True)\n\n# removing the first row since that is Harry Potter and the Philosopher's Stone, which is the movie we inquired about\nfinal = final.iloc[1:]\n\n# now this shows us which TV shows or Movies have the most number of shared actors to \n# Harry Potter and the Philosopher's Stone  \nfinal\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nNumber of shared actors\n\n\n\n\n1\nHarry Potter and the Chamber of Secrets\n37\n\n\n2\nCreating the World of Harry Potter\n36\n\n\n3\nHarry Potter and the Prisoner of Azkaban\n26\n\n\n4\nHarry Potter and the Order of the Phoenix\n24\n\n\n5\nHarry Potter and the Deathly Hallows: Part 2\n23\n\n\n...\n...\n...\n\n\n2270\nHetty Wainthropp Investigates\n1\n\n\n2271\nIntimate Relations\n1\n\n\n2272\nBrazen Hussies\n1\n\n\n2273\nBathtime\n1\n\n\n2274\nToday\n1\n\n\n\n\n2274 rows × 2 columns\n\n\n\nNow we can iterate through our data and take only the movies that have 7 common actors. This would then be our best movie recomendations\n\n# iterates through data frame and removes all Tv Shows/Movies with less than 7 common actors\nfor index, row in final.iterrows():\n    if row['Number of shared actors'] &lt; 7:\n        final.drop(index, inplace=True)\n\nFinally, we can plot our movie recomendations\n\n# using plotly to graph the data:\n\n#importing packages\nimport plotly.io as pio\nfrom plotly import express as px\npio.renderers.default=\"iframe\" \n\n\n# graphing\n# x axis is movie_or_Tv name\n# y axis is Number of shared actors\nfig = px.bar(final,\n                x=\"movie_or_TV_name\",\n                y=\"Number of shared actors\",\n                title = \"Movies/TV Shows with at least 7 actors in common\")\nfig.show()\n\n\n\n\nIt looks like the Harry Potter Franchise is a big recommendation, along with Doctor Who and Performance"
  },
  {
    "objectID": "posts/bruin/bruin.html",
    "href": "posts/bruin/bruin.html",
    "title": "Bruin",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/hw4/index.html",
    "href": "posts/hw4/index.html",
    "title": "HW 4: Simulating heat diffusion with multiple methods",
    "section": "",
    "text": "In this blog, I will explain a set by step simulation of two-dimensional heat diffusion using four methods. We will look to see which method is the quickest and most efficient\n\nStep 1: Import all Packages\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport time\nimport inspect\n\nfrom jax.experimental import sparse\nimport jax.numpy as jnp\nimport jax\n\nFor this simulation, we will use numpy, jax, and sparse as differnet methods. We will also use matplotlib to plot our simulations, and time to time how long each method of simulation takes to run.\n\nN = 101 # size of grid (n x n)\nepsilon = 0.2 # size of each step\niterations = 2700 # number of iteratons\n\n\n\nWe will set our initial conditions as well. We will use a grid size of 101 x 101 and an epsilon constant of 0.2. For each simulation, we will use 2700 iterations, marking each 300th iteration, for. a total of 9 marks for each simulation.\n\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\nFinall, we will set our initial condition of 1 unit of heat at midpoint. Notice the yellow dot in the middle\n\n\nStep 2: Simulating using matrix multiplication\nWe will start by using matrix-vector multiplication to simulate the heat diffusion in the 2D space. The vector here is created by flattening the current solution . Each iteration of the update is given by the function advance_time_matvecmul.\n\nfrom heat_equation import advance_time_matvecmul\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"\n    advances by one timestep using matrix-vector multiplication\n    Args:\n        u: N x N grid\n        epsilon:  constant\n\n    Returns:\n        N x N grid that was advanced\n    \"\"\"\n    N = u.shape[0] # gets size N from u, since u is N x N\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\nWe then define a function get_A that takes the value N as the argument and returns the corresponding matrix A.\n\nfrom heat_equation import get_A\nprint(inspect.getsource(get_A))\n\ndef get_A(N): # takes argument N and returns the corresponding matrix A\n    '''\n    Creates and returns matrix A for advance_time_matvecmul\n    \n    Parameters:\n    N : side of the matrix (N x N)\n    \n    Returns:\n    Matrix A : N x N used for advance_time_matvecmul\n    \n    '''\n    \n    \n    \n    n = N * N # \n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A= np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    \n    return A\n\n\n\nUsing get_A and advance_time_matvecmul, we will run the simulation for 2700 iterations and save every 300th iteration to be plotted. We will measure the time it takes to run this to see how long matrix multiplcation takes.\nThe code to run the simulation is below, note the time it takes to run.\nWe start by reseting out initial condition that we previewed above. We then start the timer, and create a 9 item list to hold our 9 subplots. We use get_A and advance_time_matvecmul to run the simulation, and end with plotting our 9 subplots.\n\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n\n\nstart = time.time() # gets the beginning time\n\n\nimages = [None, None, None,\n          None, None, None,\n          None, None, None] # using a 3x3 grid for images\n\n\n\nA = get_A(N) # creates the A matrix\nfor i in range(1,iterations+1): # we add +1 here since range doesn't include last number\n    u0 = advance_time_matvecmul(A, u0, epsilon) # advances one timestep\n    if i % 300 == 0: # sees if its the 300th iterations\n        images[(i//300) - 1] = u0 # adds the iteration to the list of images\nprint((time.time() - start), \"seconds\") # print time it take to exectue\n\n\n\n\n\nx, img = plt.subplots(3,3) # plots the images\nfor i in range(9):\n    if i &lt; 3:\n        img[0][i].imshow(images[i]) # plots\n    \n    elif i &lt; 6:\n        img[1][i-3].imshow(images[i])\n        \n    elif i &lt; 9:\n        img[2][i-6].imshow(images[i])\n    \n\n39.29159188270569 seconds\n\n\n\n\n\n\n\n\n\nWe note it took about 40 seconds to run this simulation. That’s long. We can look at other simulation methods to see if a more efficient method exists.\n\n\nStep 3: Simulating using Sparse matrix in JAX\nWe will now attempt to speed up our simulation using the sparse matrix in Jax. We will need to modify our get_A function slightly to work with jax. Our new function uses jnp.array and sparse.BC00 commands to return a N x N matrix to be used for advance_time_matvecmul_jax\n\nfrom heat_equation import get_sparse_A\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N):\n    \"\"\"\n    returns final - the sparsed matrix A\n    \n    Parameters:\n        N: Size of the returned matrix A (N x N)\n\n    Returns:\n        final: N x N matrix for advance_time_matvecmul\n    \"\"\"\n  \n    A = get_A(N) # gets A from above function\n    \n    new = jnp.array(A)\n    \n    final = sparse.BCOO.fromdense(new)\n    \n    return final\n\n\n\nWe will also need to modify our advance time function to be used with jax. This is a simple modification.\n\nfrom heat_equation import advance_time_matvecmul_jax\nprint(inspect.getsource(advance_time_matvecmul_jax))\n\n@jax.jit\ndef  advance_time_matvecmul_jax(A, u, epsilon):\n    \"\"\"\n    advances by one timestep using sparse matrix\n    Args:\n        A: Sparse matrix A\n        u: N x N grid \n        epsilon:  constant\n\n    Returns:\n        N x N grid that was advanced\n    \"\"\"\n    N = u.shape[0]  # gets size N from u, since u is N x N\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\nNow, using the two functions, we can run our simulation just as we had before.\n\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n\n\nstart = time.time() # gets the beginning time\n\n\nimages = [None, None, None,\n          None, None, None,\n          None, None, None] # using a 3x3 grid for images\n\n\nA = get_sparse_A(N)\nfor i in range(1,iterations+1):\n    u0 = advance_time_matvecmul_jax(A, u0, epsilon)\n    if i % 300 == 0:\n        images[(i//300) - 1] = u0\nprint((time.time() - start), \"seconds\") # print time it take to exectue\n\n\n\n\nx, img = plt.subplots(3,3) # plots the images\nfor i in range(9):\n    if i &lt; 3:\n        img[0][i].imshow(images[i]) # plots\n    \n    elif i &lt; 6:\n        img[1][i-3].imshow(images[i])\n        \n    elif i &lt; 9:\n        img[2][i-6].imshow(images[i])\n\n3.858076810836792 seconds\n\n\n\n\n\n\n\n\n\nNow, our simulation only took 3.8 seconds to run. This is 10 times faster than the original 40 seconds it took using matrix multiplcation. But we can continue testing methods to see if there’s an even faster one out there.\n\n\nStep 4: Simulating using direct operation with numpy\nWe will first pad the matrix with zeroes on all sides to create a conditional boundary. We will then flatten the matrix in a similar way as before, and then use np.roll to roll the elements of the array along a the axis. We will the finally reshape the matrix back to N x N to be used in the simulation.\nWe will use a new advance time that advances the solution by one timestep in the file.\n\nfrom heat_equation import advance_time_numpy\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    advances by one timestep using numpy techniques\n    \n    Parameters:\n        u: N x N grid\n        epsilon: constant\n\n    Returns:\n        reshaped : N x N grid that was advanced\n    \"\"\"\n    N = u.shape[0] # gets N which is from the N x N u\n\n        \n    # extends the grid with a border of zeros (padding)\n    u = np.pad(u,((1,1), (1,1)), constant_values=0).flatten()\n\n    \n    # simulates using numpy and roll\n    #  roll is \"rolls the elements of an array along a specified axis\"\n    final = u + epsilon*(np.roll(u,N+2) + np.roll(u,-(N+2)) + np.roll(u,-1)  + np.roll(u,1) - 4*u) \n    \n    \n    # reshapes back to N x N\n    reshaped = np.reshape(final,(N+2,N+2))[1:N+1,1:N+1]\n    \n    return reshaped\n\n\n\nNote we do not have a get_A for this one. Now we will run our simulation as before.\n\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n\n\nstart = time.time() # gets the beginning time\n\n\nimages = [None, None, None,\n          None, None, None,\n          None, None, None] # using a 3x3 grid for images\n\n\nfor i in range(1,iterations+1):\n    u0 = advance_time_numpy(u0, epsilon)\n    if i % 300 == 0:\n        images[(i//300) - 1] = u0\nprint((time.time() - start), \"seconds\") # print time it take to exectue\n\nx, img = plt.subplots(3,3) # plots the images\nfor i in range(9):\n    if i &lt; 3:\n        img[0][i].imshow(images[i]) # plots\n    \n    elif i &lt; 6:\n        img[1][i-3].imshow(images[i])\n        \n    elif i &lt; 9:\n        img[2][i-6].imshow(images[i])\n\n0.21892309188842773 seconds\n\n\n\n\n\n\n\n\n\nNow, this took less than 1 second! This is another huge improvement from the previous method\n\n\nStep 5: Simulating using jax\nNow, let’s use jax to do the similar using just-in-time compilation without using (sparse) matrix multiplication routines.\n\nfrom heat_equation import advance_time_jax\nprint(inspect.getsource(advance_time_jax))\n\n@jax.jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    advances by one timestep using numpy techniques\n    \n    Parameters:\n        u: N x N grid\n        epsilon: constant\n\n    Returns:\n        reshaped : N x N grid that was advanced\n    \"\"\"\n    N = u.shape[0] # gets size N\n    \n    \n    # extends the grid with a border of zeros (padding)\n    u = jnp.pad(u,((1,1), (1,1)), constant_values=0).flatten()\n    \n    \n        \n    # simulates using numpy and roll\n    #  roll is \"rolls the elements of an array along a specified axis\"\n    final = u + epsilon*(jnp.roll(u,N+2)+ jnp.roll(u,-(N+2))+ jnp.roll(u,-1) + jnp.roll(u,1) - 4*u)\n    \n    # reshapes back to N x N\n    reshaped = jnp.reshape(final,(N+2,N+2))[1:N+1,1:N+1]\n    return reshaped\n\n\n\nUsing our new advance_time_jax function, we will run the simulations as before.\n\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n\nstart = time.time() # gets the beginning time\n\n\nimages = [None, None, None,\n          None, None, None,\n          None, None, None] # using a 3x3 grid for images\n\n\n\nfor i in range(1,iterations+1):\n    u0 = advance_time_jax(u0, epsilon)\n    if i % 300 == 0:\n        images[(i//300) - 1] = u0\nprint((time.time() - start), \"seconds\")\n\nx, img = plt.subplots(3,3) # plots the images\nfor i in range(9):\n    if i &lt; 3:\n        img[0][i].imshow(images[i]) # plots\n    \n    elif i &lt; 6:\n        img[1][i-3].imshow(images[i])\n        \n    elif i &lt; 9:\n        img[2][i-6].imshow(images[i])\n\n0.08771395683288574 seconds\n\n\n\n\n\n\n\n\n\nThis time it took 0.08 seconds, more than twice as fast as the prior method!\n\n\nStep 6: Summary\nWe can see that each method showed major speed increases. The final method, jax, took less than 0.1 seconds - a huge improvment to our original 40 second matrix multiplcation method.\n\n\nStep 7: Comparisons\nMatrix multiplication was the first method we used. It was the simplest to implement since the majority of the code was provided to us in the instructions. The method was the slowest to run, taking about 40 seconds. When I was testing I noticed this code took sometiems taking even longer. The second method was with sparse matrixes. This method was much faster, taking slightly less than 4 seconds. This was 10x faster than the prior method! This implementation was fairly simple to code as well. The third method was with direct operation and used some differential equations. This was also significantly faster, taking less that 1 second to run. The final and fastess method was simulated using jax. This was super fast, taking only 0.08 seconds to run. The fourth method demonstrated how efficent jax is, and after the homework, I woudld recommend anyone to default to the jax methods when running similar projects."
  },
  {
    "objectID": "posts/hw 5/index.html",
    "href": "posts/hw 5/index.html",
    "title": "HW 5: Image Classifying of Dogs/Cats using Keras",
    "section": "",
    "text": "We will teach a machine learning algorithm to distinguish between pictures of dogs and pictures of cats. To do this, we will use tensorflow and keras. We will test several methods to see which method is the most accurate.\n\nStep 1: Importing necessary packages to use in the project\n\n# importing necessary packages\n\nimport os  \nimport numpy as np \n\n# setting the Keras backend to TensorFlow\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\n# importing tensflor's Keras module\nfrom tensorflow import keras  \nfrom tensorflow.keras import utils, layers, models  # for building and training neural networks\nimport tensorflow_datasets as tfds\n\n\nfrom tensorflow import expand_dims  # expanding dimensions of a tensor\nfrom tensorflow import data as tf_data  # tensorflow data module for building input pipelines\n\n# importing Matplotlib for data visualization\nfrom matplotlib import pyplot as plt\n\n# importing the random module for generating random numbers\nimport random\n\n\n\nStep 2: Import Data\nWe’ll use a sample data set from Kaggle that contains labeled images of cats and dogs. Its imported below and split between train data set, validation data set, and test data set.\n\n# creates our training, validation, and testing data sets\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\nFrom this we see how our data was split up!\nNow, we will resize our data using the next two code blocks. The dataset contains images of different sizes, so we resize them to a fixed size of 150x150.\n\n# resizing our data sets\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y)) # using lambda function to resize the train_ds code\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y)) # using lambda function to resize the validation_ds code\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y)) # using lambda function to resize teh test_ds code\n\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\n\n\nStep 3: Visualize some data\nBelow we have a function to vidualize random images from the data set. It will generate 3 random images of cats, and 3 random images of dogs. We will use matplotlib to visualize the images from the given data\n\n# writing a function to randomly visualize 3 images of cats and 3 images of dogs\n\n#show 3 cats, 3  dogs\ndef visualize(dataset):\n    #plt.figure(figsize=(10, 6))\n    dogs = 0\n    cats = 0\n    n=random.randint(1,20)  # makes it so we have a random image of dogs and cats\n    for img, lbl in dataset.take(1): # iterates through the images and their corresponding label\n        for i in range(32+n):\n            i = i+n\n            if int(lbl[i]) == 1 and dogs &lt; 3:  # repeats while the number of dogs pulled is less than 3\n                ax = plt.subplot(2, 3, dogs+4) \n                plt.imshow(img[i].numpy().astype(\"uint8\"))\n                plt.axis(\"off\") # removes axis numbering\n                dogs = dogs + 1\n            if int(lbl[i]) == 0 and cats &lt; 3: # repeates while the number of cats pulled is less than 3\n                ax = plt.subplot(2, 3, cats+1) \n                plt.imshow(img[i].numpy().astype(\"uint8\"))\n                plt.axis(\"off\") # removes axis numbering\n                cats = cats + 1\n          \n\n\n\nvisualize(train_ds) # use the above function\n\n2024-03-03 16:41:06.283929: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Check Dog/Cat Frequencies\nWith the code below, we will iterator through the data and check to see how many images of dogs and cats we have. Note that each image has a label of 0 or 1. 0 correpsonds to cats while 1 corresponds to dogs.\n\n# check label frequencies\n\n\n#The following line of code will create an iterator called labels_iterator.\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\n\n\n# computes the number of dogs and cats in our data. The label 0 corresponds to cats, and 1 corresponds to dogs\n\ncats = 0\ndogs = 0\nfor label in labels_iterator:\n    if label == 0: # label zero corresponds to cats\n        cats = cats + 1\n    if label ==1: # 1 corresponds to dogs\n        dogs = dogs + 1\naccuracy = dogs/(cats+dogs) # assume always guessing dogs out of all the cats and dogs\nrounded = round((accuracy*100),2) # rounds to 2 decimal places\n\nprint(f\"There are {cats} cats and {dogs} dogs\")\nprint(f\"If we always guess dogs for each image the baseline will be {rounded}% accurate\")\n\nThere are 4637 cats and 4668 dogs\nIf we always guess dogs for each image the baseline will be 50.17% accurate\n\n\nThis gives us our baseline of 50.17% accuracy. This means that if we guessed dogs for each and every image in the data set, we would be 50.17% accurate since 50.17% of the images in the dataset are dogs.\n\n\nStep 5: keras.Sequential model\nWe will start our models with a keras.Sequential model using several layers: Conv2D, MaxPooling2D, Flatten, Dense, and Dropout. We can experiment with the ordering of the layers, the frequency of the layers, and the size of the layers to alter our accuracy. The most succesful is seen below. There is also a graph comparing the training and validation sets.\n\n# first model\n\n\nfirst_model = models.Sequential([\n    \n    # input layer with shape (150, 150, 3), matches the size of the input images\n    layers.Input((150, 150, 3)), #this layer needs to be same size as image\n\n    \n    # Convolutional2D layer with 32 filters, each of size (3, 3) - relu activation\n    layers.Conv2D(32, (3, 3), activation='relu'),\n\n    \n    \n    # Maxpooling2D layer with pool size (2, 2)\n    layers.MaxPooling2D((2, 2)),\n\n    \n     # Convolutional2D layer with 32 filters, each of size (3, 3) - relu activation\n    layers.Conv2D(32, (3, 3), activation='relu'),\n\n    \n    # Maxpooling2D layer with pool size (2, 2)\n    layers.MaxPooling2D((2, 2)),\n    \n     # Convolutional2D layer with 64 filters, each of size (3, 3) - relu activation\n    layers.Conv2D(64, (3, 3), activation='relu'),\n\n    \n    # Dropout layer \n    layers.Dropout(0.2),\n\n      # Maxpooling2D layer with pool size (2, 2)\n    layers.MaxPooling2D((2, 2)),\n\n    #flatten layers\n    layers.Flatten(),\n\n    layers.Dense(64, activation='relu'),\n\n    layers.Dense(1) #the layer thats outputted\n])\n\nfirst_model.compile(optimizer='adam',\n              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nhistory1 = first_model.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 36s 246ms/step - loss: 5.1396 - accuracy: 0.5810 - val_loss: 0.5930 - val_accuracy: 0.6268\nEpoch 2/20\n146/146 [==============================] - 36s 246ms/step - loss: 0.5496 - accuracy: 0.7003 - val_loss: 0.5361 - val_accuracy: 0.6887\nEpoch 3/20\n146/146 [==============================] - 36s 247ms/step - loss: 0.4738 - accuracy: 0.7595 - val_loss: 0.5089 - val_accuracy: 0.7124\nEpoch 4/20\n146/146 [==============================] - 36s 249ms/step - loss: 0.4098 - accuracy: 0.7985 - val_loss: 0.5640 - val_accuracy: 0.6866\nEpoch 5/20\n146/146 [==============================] - 37s 256ms/step - loss: 0.3538 - accuracy: 0.8356 - val_loss: 0.5650 - val_accuracy: 0.7352\nEpoch 6/20\n146/146 [==============================] - 36s 245ms/step - loss: 0.3288 - accuracy: 0.8487 - val_loss: 0.8084 - val_accuracy: 0.6853\nEpoch 7/20\n146/146 [==============================] - 36s 248ms/step - loss: 0.2932 - accuracy: 0.8641 - val_loss: 0.7522 - val_accuracy: 0.7231\nEpoch 8/20\n146/146 [==============================] - 37s 252ms/step - loss: 0.2703 - accuracy: 0.8785 - val_loss: 0.6786 - val_accuracy: 0.7403\nEpoch 9/20\n146/146 [==============================] - 39s 271ms/step - loss: 0.2836 - accuracy: 0.8735 - val_loss: 0.6400 - val_accuracy: 0.7463\nEpoch 10/20\n146/146 [==============================] - 39s 264ms/step - loss: 0.2461 - accuracy: 0.8898 - val_loss: 0.7905 - val_accuracy: 0.7347\nEpoch 11/20\n146/146 [==============================] - 38s 257ms/step - loss: 0.2433 - accuracy: 0.8943 - val_loss: 0.7968 - val_accuracy: 0.7261\nEpoch 12/20\n146/146 [==============================] - 38s 258ms/step - loss: 0.2060 - accuracy: 0.9149 - val_loss: 0.7978 - val_accuracy: 0.7412\nEpoch 13/20\n146/146 [==============================] - 38s 260ms/step - loss: 0.2168 - accuracy: 0.9091 - val_loss: 0.7550 - val_accuracy: 0.7425\nEpoch 14/20\n146/146 [==============================] - 37s 253ms/step - loss: 0.1868 - accuracy: 0.9184 - val_loss: 0.7612 - val_accuracy: 0.7145\nEpoch 15/20\n146/146 [==============================] - 35s 243ms/step - loss: 0.1690 - accuracy: 0.9270 - val_loss: 0.8398 - val_accuracy: 0.7416\nEpoch 16/20\n146/146 [==============================] - 35s 239ms/step - loss: 0.2053 - accuracy: 0.9121 - val_loss: 0.8432 - val_accuracy: 0.7339\nEpoch 17/20\n146/146 [==============================] - 38s 257ms/step - loss: 0.1618 - accuracy: 0.9309 - val_loss: 0.9763 - val_accuracy: 0.7128\nEpoch 18/20\n146/146 [==============================] - 35s 243ms/step - loss: 0.1459 - accuracy: 0.9428 - val_loss: 0.8642 - val_accuracy: 0.7304\nEpoch 19/20\n146/146 [==============================] - 35s 238ms/step - loss: 0.1473 - accuracy: 0.9411 - val_loss: 0.9056 - val_accuracy: 0.7322\nEpoch 20/20\n146/146 [==============================] - 35s 236ms/step - loss: 0.1535 - accuracy: 0.9367 - val_loss: 0.9860 - val_accuracy: 0.7339\n\n\n\nplt.plot(history1.history[\"accuracy\"], label = \"training\", color = \"red\")\nplt.plot(history1.history[\"val_accuracy\"], label = \"validation\", color = \"green\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\", xticks=np.arange(21))\nplt.legend()\n\n\n\n\n\n\n\n\nThe accuracy of the first model is around 70%. This is significantly better than baseline (about 20%). There is eveidence for overfitting since the training accuracy rose to 93% and the validation accuracy was only at 70%. This is also evident on the graph with the seperation between the training and validation sets.\n\n\nStep 6: Model with Data Augmentation\nNow, we’re going to make the model better by adding some changes to the omages it learns from. This means showing it images that are a bit different, like flipping a cat image or turning it around. Even with these changes, the model can still recognize it’s a cat. By doing this, we help the model get better at understanding important features in pictures, no matter how they’re positioned or rotated. This will hopefully make our model more accurate.\nWe will start by defining two functions, flip and rotation. Flip will randomly flip the images while rotation will randomly rotate the images. Examples are shown below.\n\ndef flip():\n    '''\n    No paremeters. Returns 9 flipped versions of a random image from the dataset\n    '''\n    \n    # Loop over train_ds data\n    for image, _ in train_ds.take(1):\n        plt.figure(figsize=(10, 10)) # creates a subplot for visualizing images\n\n        n=random.randint(1,20) # gets a random image to test the rotation on\n\n        first_image = image[n] #the random image\n        ax = plt.subplot(3, 3, 1)\n        plt.imshow(first_image / 255) #plot the first image normally\n        plt.axis('off')\n        for i in range(8):\n            ax = plt.subplot(3, 3, i + 2) \n            augmented_image = layers.RandomFlip()(expand_dims(first_image, 0)) #transform the image using flipping\n            plt.imshow(augmented_image[0] / 255)\n            plt.axis('off')\n\n\ndef rotation():\n    '''\n    No paremeters. Returns 9 rotated versions of a random image from the dataset\n    '''\n    \n    \n    # Loop over train_ds data\n    for image, _ in train_ds.take(1):\n        plt.figure(figsize=(10, 10)) # creates a subplot for visualizing images\n    \n        n=random.randint(1,20) # gets a random image to test the rotation on\n    \n        first_image = image[n]\n        ax = plt.subplot(3, 3, 1)\n        plt.imshow(first_image / 255) # plots the first image normally\n        plt.axis('off')\n        for i in range(8):\n            ax = plt.subplot(3, 3, i + 2)\n            augmented_image = layers.RandomRotation(0.2)(expand_dims(first_image, 0)) # transofmr sthe image using rotation\n            plt.imshow(augmented_image[0] / 255)\n            plt.axis('off')\n\n\nflip()\n\n\n\n\n\n\n\n\n\nrotation()\n\n\n\n\n\n\n\n\nNow we will make our second model using data agumentation. Once again, to get the best results we need to experiment with different amounts of layers, adding more dropout layers, using bigger kernel sizes, and using different activation functions. This seems to be the best model I could create.\n\nsecond_model = models.Sequential([\n    \n     # Input layer with shape (150, 150, 3), matching the size of the input images\n    layers.Input((150, 150, 3)),\n    \n     # Randomly flip the input images horizontally or vertically \n    layers.RandomFlip(),\n    \n     # Randomly rotates the input images\n    layers.RandomRotation(0.2),\n    \n    # convolutional2D layer with 32 filters, each of size (3, 3)- relu activation\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    \n     # Maxpooling2D layer with pool size (2, 2) \n    layers.MaxPooling2D((2, 2)),\n    \n    # convolutional2D layer with 32 filters, each of size (3, 3)- relu activation\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    \n    # Maxpooling2D layer with pool size (2, 2) \n    layers.MaxPooling2D((2, 2)),\n    \n    layers.Conv2D(64, (3, 3), activation='relu'),\n    # convolutional2D layer with 64 filters, each of size (3, 3)- relu activation\n    \n     # Maxpooling2D layer with pool size (2, 2) \n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(), # flattens layers\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1)\n])\n\nsecond_model.compile(optimizer='adam',\n              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nhistory2 = second_model.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 37s 252ms/step - loss: 5.5375 - accuracy: 0.5085 - val_loss: 0.6789 - val_accuracy: 0.5181\nEpoch 2/20\n146/146 [==============================] - 36s 245ms/step - loss: 0.6796 - accuracy: 0.5272 - val_loss: 0.6669 - val_accuracy: 0.5576\nEpoch 3/20\n146/146 [==============================] - 36s 245ms/step - loss: 0.6669 - accuracy: 0.5455 - val_loss: 0.6565 - val_accuracy: 0.5830\nEpoch 4/20\n146/146 [==============================] - 36s 247ms/step - loss: 0.6527 - accuracy: 0.5802 - val_loss: 0.6496 - val_accuracy: 0.6066\nEpoch 5/20\n146/146 [==============================] - 36s 245ms/step - loss: 0.6480 - accuracy: 0.5838 - val_loss: 0.6448 - val_accuracy: 0.5778\nEpoch 6/20\n146/146 [==============================] - 37s 251ms/step - loss: 0.6421 - accuracy: 0.5995 - val_loss: 0.6403 - val_accuracy: 0.6268\nEpoch 7/20\n146/146 [==============================] - 37s 252ms/step - loss: 0.6443 - accuracy: 0.5969 - val_loss: 0.6302 - val_accuracy: 0.5860\nEpoch 8/20\n146/146 [==============================] - 40s 271ms/step - loss: 0.6319 - accuracy: 0.6155 - val_loss: 0.6294 - val_accuracy: 0.6062\nEpoch 9/20\n146/146 [==============================] - 38s 260ms/step - loss: 0.6269 - accuracy: 0.6256 - val_loss: 0.6233 - val_accuracy: 0.6376\nEpoch 10/20\n146/146 [==============================] - 36s 248ms/step - loss: 0.6137 - accuracy: 0.6389 - val_loss: 0.6076 - val_accuracy: 0.6105\nEpoch 11/20\n146/146 [==============================] - 36s 247ms/step - loss: 0.6113 - accuracy: 0.6348 - val_loss: 0.6109 - val_accuracy: 0.6217\nEpoch 12/20\n146/146 [==============================] - 36s 247ms/step - loss: 0.6051 - accuracy: 0.6470 - val_loss: 0.6064 - val_accuracy: 0.6290\nEpoch 13/20\n146/146 [==============================] - 36s 247ms/step - loss: 0.6065 - accuracy: 0.6470 - val_loss: 0.5921 - val_accuracy: 0.6367\nEpoch 14/20\n146/146 [==============================] - 36s 249ms/step - loss: 0.5914 - accuracy: 0.6665 - val_loss: 0.5962 - val_accuracy: 0.6518\nEpoch 15/20\n146/146 [==============================] - 40s 273ms/step - loss: 0.5926 - accuracy: 0.6624 - val_loss: 0.5635 - val_accuracy: 0.6857\nEpoch 16/20\n146/146 [==============================] - 37s 254ms/step - loss: 0.5691 - accuracy: 0.6851 - val_loss: 0.5563 - val_accuracy: 0.7291\nEpoch 17/20\n146/146 [==============================] - 37s 252ms/step - loss: 0.5596 - accuracy: 0.6920 - val_loss: 0.5326 - val_accuracy: 0.7094\nEpoch 18/20\n146/146 [==============================] - 36s 249ms/step - loss: 0.5606 - accuracy: 0.6925 - val_loss: 0.5396 - val_accuracy: 0.7025\nEpoch 19/20\n146/146 [==============================] - 36s 247ms/step - loss: 0.5513 - accuracy: 0.6977 - val_loss: 0.5153 - val_accuracy: 0.7210\nEpoch 20/20\n146/146 [==============================] - 36s 248ms/step - loss: 0.5448 - accuracy: 0.7042 - val_loss: 0.5402 - val_accuracy: 0.7059\n\n\n\nplt.plot(history2.history[\"accuracy\"], label = \"training\", color = \"red\")\nplt.plot(history2.history[\"val_accuracy\"], label = \"validation\", color = \"green\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\", xticks=np.arange(21))\nplt.legend()\n\n\n\n\n\n\n\n\nThe validation accuracy ended up being about 70%. This is pretty much the smae as the first model, but we can see by the graph that there’s a big decrease in overfitting. The training and validation sets trail one another and end up being very close to one another in accuracy. So even though this model didn’t necessarily show more accuracy than the first model, the less overfitting is a good sign that this model is still an improvement.\n\n\nStep 7: Method 3 Data Preprocessing\nSometimes, it’s good to change how the model sees the data. For example, instead of colors going from 0 to 255, it’s easier for the model if they go from 0 to 1 or even -1 to 1. By doing this, the model can pay more attention to the important details in the pictures during learning, and it won’t have to work as hard to understand different color ranges. This is kind of similiar to the second method.\nThe following code will create a preprocessing layer called preprocessor which you can slot into your model pipeline. Then, we will implement the third model.\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\n\nthird_model = models.Sequential([\n    preprocessor,\n    layers.RandomFlip(),\n    layers.RandomRotation(0.2),\n\n     # convolutional2D layer with 32 filters, each of size (3, 3)- relu activation\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    \n    \n     # Maxpooling2D layer with pool size (2, 2) \n    layers.MaxPooling2D((2, 2)),\n\n     # convolutional2D layer with 32 filters, each of size (3, 3)- relu activation\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    \n     # Maxpooling2D layer with pool size (2, 2) \n    layers.MaxPooling2D((2, 2)),\n\n     # convolutional2D layer with 32 filters, each of size (3, 3)- relu activation\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    \n     # Maxpooling2D layer with pool size (2, 2) \n    layers.MaxPooling2D((2, 2)),\n\n     # convolutional2D layer with 32 filters, each of size (3, 3)- relu activation\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    \n    \n     # Maxpooling2D layer with pool size (2, 2) \n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(), # flattens layers\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation=\"sigmoid\")\n])\n\nthird_model.compile(optimizer='adam',\n              loss=keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])\n\n\nhistory3 = third_model.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 44s 299ms/step - loss: 0.6774 - accuracy: 0.5675 - val_loss: 0.6414 - val_accuracy: 0.6217\nEpoch 2/20\n146/146 [==============================] - 39s 270ms/step - loss: 0.6278 - accuracy: 0.6473 - val_loss: 0.5693 - val_accuracy: 0.7102\nEpoch 3/20\n146/146 [==============================] - 40s 271ms/step - loss: 0.5924 - accuracy: 0.6794 - val_loss: 0.5621 - val_accuracy: 0.7137\nEpoch 4/20\n146/146 [==============================] - 40s 271ms/step - loss: 0.5779 - accuracy: 0.6945 - val_loss: 0.5415 - val_accuracy: 0.7248\nEpoch 5/20\n146/146 [==============================] - 40s 275ms/step - loss: 0.5534 - accuracy: 0.7205 - val_loss: 0.5363 - val_accuracy: 0.7313\nEpoch 6/20\n146/146 [==============================] - 40s 275ms/step - loss: 0.5408 - accuracy: 0.7247 - val_loss: 0.5196 - val_accuracy: 0.7451\nEpoch 7/20\n146/146 [==============================] - 38s 257ms/step - loss: 0.5301 - accuracy: 0.7367 - val_loss: 0.5099 - val_accuracy: 0.7519\nEpoch 8/20\n146/146 [==============================] - 38s 263ms/step - loss: 0.5211 - accuracy: 0.7392 - val_loss: 0.5025 - val_accuracy: 0.7558\nEpoch 9/20\n146/146 [==============================] - 37s 257ms/step - loss: 0.5066 - accuracy: 0.7459 - val_loss: 0.5011 - val_accuracy: 0.7666\nEpoch 10/20\n146/146 [==============================] - 37s 255ms/step - loss: 0.4857 - accuracy: 0.7661 - val_loss: 0.4826 - val_accuracy: 0.7678\nEpoch 11/20\n146/146 [==============================] - 37s 255ms/step - loss: 0.4715 - accuracy: 0.7708 - val_loss: 0.5164 - val_accuracy: 0.7537\nEpoch 12/20\n146/146 [==============================] - 37s 255ms/step - loss: 0.4631 - accuracy: 0.7736 - val_loss: 0.4623 - val_accuracy: 0.7790\nEpoch 13/20\n146/146 [==============================] - 40s 276ms/step - loss: 0.4468 - accuracy: 0.7888 - val_loss: 0.4717 - val_accuracy: 0.7721\nEpoch 14/20\n146/146 [==============================] - 41s 282ms/step - loss: 0.4388 - accuracy: 0.7889 - val_loss: 0.4295 - val_accuracy: 0.7997\nEpoch 15/20\n146/146 [==============================] - 40s 272ms/step - loss: 0.4229 - accuracy: 0.8003 - val_loss: 0.4341 - val_accuracy: 0.8001\nEpoch 16/20\n146/146 [==============================] - 39s 270ms/step - loss: 0.4155 - accuracy: 0.8042 - val_loss: 0.4280 - val_accuracy: 0.8048\nEpoch 17/20\n146/146 [==============================] - 38s 264ms/step - loss: 0.4069 - accuracy: 0.8130 - val_loss: 0.4048 - val_accuracy: 0.8186\nEpoch 18/20\n146/146 [==============================] - 194s 1s/step - loss: 0.3905 - accuracy: 0.8189 - val_loss: 0.4465 - val_accuracy: 0.7975\nEpoch 19/20\n146/146 [==============================] - 38s 260ms/step - loss: 0.3883 - accuracy: 0.8232 - val_loss: 0.3984 - val_accuracy: 0.8169\nEpoch 20/20\n146/146 [==============================] - 38s 258ms/step - loss: 0.3733 - accuracy: 0.8337 - val_loss: 0.3863 - val_accuracy: 0.8332\n\n\n\nplt.plot(history3.history[\"accuracy\"], label = \"training\", color = \"red\")\nplt.plot(history3.history[\"val_accuracy\"], label = \"validation\", color = \"green\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\", xticks=np.arange(21))\nplt.legend()\n\n\n\n\n\n\n\n\nWe can see we reached an accuracy of about 82%! This has show a 10% increase from the first and second models. We are beginning to make huge progress in our models. We can see there is even less overfitting here than there was in the second model, with the training and validation graphs being almost the same towards the end.\n\n\nStep 8: Model 4 Transfer Learning\nUntil now, we’ve been teaching models to tell the difference between cats and dogs right from the beginning. But sometimes, others have already trained a model that recognizes things in pictures, and it might know useful patterns. For instance, people train models for recognizing different things in images.\nTo try this, we first use an existing “base model,” include it in a complete model for our specific job, and then train that combined model.\nThe following code downloads MobileNetV3Large and configures it as a layer that can be included in the model.\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\n\nfourthmodel = models.Sequential([\n    \n    # inputted layer with shape (150, 150, 3), same size of the input images\n    layers.Input((150, 150, 3)),\n    layers.RandomFlip(), # randomly flips the images\n    layers.RandomRotation(0.2), # randomly rotates the images\n    base_model_layer,\n    \n    # globalmaxpooling2D layer\n    layers.GlobalMaxPooling2D(),\n    # droupout  with a dropout rate of 0.5 \n    layers.Dropout(0.5),\n    layers.Dense(1)\n])\n\n# Compiling the model with an Adam optimizer\nfourthmodel.compile(optimizer='adam',\n              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nfourthmodel.build(train_ds)\n\nfourthmodel.summary()\n\nModel: \"sequential_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_5 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_5 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n model_5 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n global_max_pooling2d_2 (Gl  (None, 960)               0         \n obalMaxPooling2D)                                               \n                                                                 \n dropout_6 (Dropout)         (None, 960)               0         \n                                                                 \n dense_10 (Dense)            (None, 1)                 961       \n                                                                 \n=================================================================\nTotal params: 2997313 (11.43 MB)\nTrainable params: 961 (3.75 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\n\n\n\nhistory4 = fourthmodel.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 31s 199ms/step - loss: 2.3508 - accuracy: 0.7531 - val_loss: 0.2859 - val_accuracy: 0.9544\nEpoch 2/20\n146/146 [==============================] - 29s 201ms/step - loss: 1.0780 - accuracy: 0.8502 - val_loss: 0.2069 - val_accuracy: 0.9665\nEpoch 3/20\n146/146 [==============================] - 30s 203ms/step - loss: 0.8550 - accuracy: 0.8666 - val_loss: 0.1756 - val_accuracy: 0.9639\nEpoch 4/20\n146/146 [==============================] - 28s 189ms/step - loss: 0.6457 - accuracy: 0.8779 - val_loss: 0.1655 - val_accuracy: 0.9600\nEpoch 5/20\n146/146 [==============================] - 29s 199ms/step - loss: 0.5234 - accuracy: 0.8877 - val_loss: 0.1211 - val_accuracy: 0.9669\nEpoch 6/20\n146/146 [==============================] - 28s 192ms/step - loss: 0.4872 - accuracy: 0.8804 - val_loss: 0.1082 - val_accuracy: 0.9652\nEpoch 7/20\n146/146 [==============================] - 28s 195ms/step - loss: 0.4099 - accuracy: 0.8859 - val_loss: 0.1014 - val_accuracy: 0.9647\nEpoch 8/20\n146/146 [==============================] - 28s 189ms/step - loss: 0.3417 - accuracy: 0.8904 - val_loss: 0.0928 - val_accuracy: 0.9656\nEpoch 9/20\n146/146 [==============================] - 27s 188ms/step - loss: 0.3346 - accuracy: 0.8892 - val_loss: 0.0902 - val_accuracy: 0.9626\nEpoch 10/20\n146/146 [==============================] - 28s 189ms/step - loss: 0.3362 - accuracy: 0.8788 - val_loss: 0.1238 - val_accuracy: 0.9587\nEpoch 11/20\n146/146 [==============================] - 28s 195ms/step - loss: 0.3400 - accuracy: 0.8801 - val_loss: 0.0925 - val_accuracy: 0.9652\nEpoch 12/20\n146/146 [==============================] - 28s 189ms/step - loss: 0.3278 - accuracy: 0.8811 - val_loss: 0.1033 - val_accuracy: 0.9678\nEpoch 13/20\n146/146 [==============================] - 28s 193ms/step - loss: 0.3311 - accuracy: 0.8826 - val_loss: 0.0894 - val_accuracy: 0.9626\nEpoch 14/20\n146/146 [==============================] - 27s 186ms/step - loss: 0.3218 - accuracy: 0.8848 - val_loss: 0.0918 - val_accuracy: 0.9647\nEpoch 15/20\n146/146 [==============================] - 27s 186ms/step - loss: 0.3229 - accuracy: 0.8848 - val_loss: 0.1121 - val_accuracy: 0.9604\nEpoch 16/20\n146/146 [==============================] - 27s 188ms/step - loss: 0.3198 - accuracy: 0.8855 - val_loss: 0.1091 - val_accuracy: 0.9643\nEpoch 17/20\n146/146 [==============================] - 27s 188ms/step - loss: 0.3475 - accuracy: 0.8790 - val_loss: 0.0946 - val_accuracy: 0.9656\nEpoch 18/20\n146/146 [==============================] - 27s 188ms/step - loss: 0.3298 - accuracy: 0.8837 - val_loss: 0.1022 - val_accuracy: 0.9609\nEpoch 19/20\n146/146 [==============================] - 28s 189ms/step - loss: 0.3472 - accuracy: 0.8783 - val_loss: 0.0979 - val_accuracy: 0.9647\nEpoch 20/20\n146/146 [==============================] - 27s 188ms/step - loss: 0.3429 - accuracy: 0.8834 - val_loss: 0.0934 - val_accuracy: 0.9712\n\n\n\nplt.plot(history4.history[\"accuracy\"], label = \"training\", color = \"red\")\nplt.plot(history4.history[\"val_accuracy\"], label = \"validation\", color = \"green\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\", xticks=np.arange(21))\nplt.legend()\n\n\n\n\n\n\n\n\nThis model has about a 96% validation accuracy which is a big improvment to the other models. Here, overfitting is not present. Instead, the validation data is more accurate than the training validation. I’m not sure why that is, but I’m happy its at very accurate!\n\n\nStep 9: Final Testing\n\nfourthmodel.evaluate(test_ds, verbose=2)\n\n37/37 - 6s - loss: 0.1222 - accuracy: 0.9604 - 6s/epoch - 173ms/step\n\n\n[0.12224007397890091, 0.9604471325874329]\n\n\nThe final evaluation shows a 96% accuracy. All things considered this is very accurate!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome! This is Nico TestingW!!!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog2",
    "section": "",
    "text": "HW 5: Image Classifying of Dogs/Cats using Keras\n\n\n\n\n\n\nweek 8\n\n\nHomework\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nNico Morrone\n\n\n\n\n\n\n\n\n\n\n\n\nHW 4: Simulating heat diffusion with multiple methods\n\n\n\n\n\n\nweek 7\n\n\nHomework\n\n\n\n\n\n\n\n\n\nFeb 10, 2024\n\n\nNico Morrone\n\n\n\n\n\n\n\n\n\n\n\n\nHW 3 A Fun Message Bank!\n\n\n\n\n\n\nweek 5\n\n\nHomework\n\n\n\n\n\n\n\n\n\nFeb 10, 2024\n\n\nNico Morrone\n\n\n\n\n\n\n\n\n\n\n\n\nHW 2 Scrapy Movies Recomendations\n\n\n\n\n\n\nweek 4\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nNico Morrone\n\n\n\n\n\n\n\n\n\n\n\n\nHW 1 - Analyzing and Comparing Temps of Various Countries\n\n\n\n\n\n\nweek 3\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nNico Morrone\n\n\n\n\n\n\n\n\n\n\n\n\nHW 0\n\n\n\n\n\n\nweek 0\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nNico Morrone\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\nBruin\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\nNico\n\n\n\n\n\n\nNo matching items"
  }
]