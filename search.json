[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/hw1/index.html",
    "href": "posts/hw1/index.html",
    "title": "HW 1 - Analyzing and Comparing Temps of Various Countries",
    "section": "",
    "text": "Step 1:\nimport all necessary packages\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport plotly.express as px\n\n\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\n\nStep 2\n\nImport the csv files: temps, countries, and stations\nThen, we will use the prepare_df function to reorganize the temps csv file. We can preview the temps file after we run it through the function to see the changes.\n\n\ntemps = pd.read_csv(\"temps.csv\")\n\ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries = pd.read_csv(countries_url)\n\nstations = pd.read_csv(\"station-metadata.csv\")\n\n\ndef prepare_df(df):\n    \"\"\"\n    Prepares the inputed dataframe by reorganzing the data.\n\n    Parameters:\n    - df: inputted dataframe containing temperature data.\n\n    Returns:\n    df: New dataframe with columns 'ID', 'Year', 'Month', and 'Temp'.\n                  The 'Month' column represents the month in integer form, and the 'Temp' column\n                  represents the temperature values\n    \"\"\"\n    \n    \n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\n\ntemps = prepare_df(temps)\ntemps\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\n\n\n\n\n0\nACW00011604\n1961\n1\n-0.89\n\n\n1\nACW00011604\n1961\n2\n2.36\n\n\n2\nACW00011604\n1961\n3\n4.72\n\n\n3\nACW00011604\n1961\n4\n7.73\n\n\n4\nACW00011604\n1961\n5\n11.28\n\n\n...\n...\n...\n...\n...\n\n\n13992657\nZIXLT622116\n1970\n8\n15.40\n\n\n13992658\nZIXLT622116\n1970\n9\n20.40\n\n\n13992659\nZIXLT622116\n1970\n10\n20.30\n\n\n13992660\nZIXLT622116\n1970\n11\n21.30\n\n\n13992661\nZIXLT622116\n1970\n12\n21.50\n\n\n\n\n13992662 rows × 4 columns\n\n\n\nWe can see that now that the Month column now reflects all months, instead of each month being in its own column\n\n\nStep 3:\nUsing SQLITE, create a data base called “temps.db”. We link the temps file to “temperatures” We link the stations file to “stations” We link the countries file to “countries”\nNow all three data sets live within our temps.db data base\nWe make sure to close our connection to the database once we created it.\n\nwith sqlite3.connect(\"temps.db\") as conn:\n    temps.to_sql(\"temperatures\", conn, if_exists = \"replace\", index = False)\n    stations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n    countries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n\nconn.close()\n#closes connection\n\n\n\nStep 4:\nImport the query_climate_database function. This function takes 5 parameters: db_file, country, year_begin, year_end, & month. - db_file is the name of our database file. In this case, its “temps.db” - country is the name of the country we are inquiring about. - year_begin is the beginning of the range of years we are inquiring about - year_end is the ending of the range of years we are inquiring about - month is the month number we are inquring about (example: January is 1)\n\nwe use SELECT to select the specified columns, JOIN to put the columns from multiple tables next to one another, and WHERE to specify conditions using the parameters\n\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Query a climate database to retrieve temperature data for a specific country, year range, and month.\n\n    Parameters:\n    - db_file: database file.\n    - country: name of the country for which climate data is requested.\n    - year_begin: starting year of the desired temperature data.\n    - year_end: ending year of the desired temperature data.\n    - month: month for which temperature data is requested.\n\n    Returns:\n    pd: dataframe containing weather station information ('NAME', 'LATITUDE', 'LONGITUDE', 'COUNTRY'),\n                  along with temperature data ('YEAR', 'MONTH', 'ID', 'TEMP')\n                  \n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    cursor = conn.cursor()\n    #cursor.execute(f'SELECT C.Name, SUBSTR(T.ID,1,2) FROM temperatures T LEFT JOIN countries C on T.\"FIPS 10-4\" = C.\"FIPS 10-4\" WHERE C.Name=\"{country}\"')\n    cursor.execute(f'SELECT C.Name, SUBSTR(T.ID,1,2) FROM temperatures T JOIN countries C WHERE C.Name=\"{country}\" AND SUBSTR(T.ID,1,2) = C.\"FIPS 10-4\"')  \n\n    result = [cursor.fetchone() for i in range(1)]  # get just the first  result\n    #this gives us Name and first two letters\n    \n    letters = result[0][1] # this gives us IN\n    \n    x = cursor.execute(f'SELECT T.Year, T.Temp, T.ID FROM temperatures T WHERE SUBSTR(T.ID,1,2)=\"{letters}\" AND T.Year &lt;= {year_end} AND T.Year &gt;= {year_begin} AND T.Month={month}')\n    # we now have year and temp for the given inputs\n    #still need name of weatherpeerson and latitude/longitude\n\n    cursor = conn.cursor()\n    y=cursor.execute(f'SELECT S.NAME, S.LATITUDE, S.LONGITUDE, S.ID FROM stations S LEFT JOIN temperatures T on T.ID = S.ID WHERE SUBSTR(S.ID,1,2)=\"{letters}\"')\n    \n    \n    \n\n    # putting all info into data frame\n    df = pd.DataFrame(x, columns = [\"YEAR\", \"TEMP\", \"ID\"])\n    df2 = pd.DataFrame(y, columns = [\"NAME\", \"LATITUDE\", \"LONGITUDE\", \"ID\"])\n    df2[\"COUNTRY\"] = country\n    #print(cursor.fetchall())\n    merged = pd.merge(df, df2, on='ID')\n    dropped = merged.dropna()\n    final = dropped.drop_duplicates()\n    \n    \n\n    # Reorder columns\n    reordered = final[[\"NAME\", \"LATITUDE\", \"LONGITUDE\", \"COUNTRY\", \"YEAR\", \"ID\", \"TEMP\"]]\n    \n    #changing the ID column to display the month instead\n    reordered[\"ID\"] = month\n    final2 = reordered.rename(columns={'ID': 'MONTH'})\n    \n    #this resets the indexes\n    reset = final2.reset_index(drop=True)\n\n    reset2 = reset.rename(columns={'COUNTRY': 'Country', 'YEAR': 'Year', 'MONTH': 'Month', 'TEMP': 'Temp'})\n\n\n    conn.close()\n    return reset2\n\n\n\n\nnow test our function\n\n\ntest = query_climate_database(\"temps.db\", \"India\", 1980, 2020, 1)\ntest\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\nOur function worked! It gave us the temepratures for all stations in India. The temperatures are from January, measured between 1980 and 202\n\n\nStep 5:\nNow that we can easily pull data using our function, we can make interactive plots of our data! We will use plotly to create a geographic graph of the weather stations.\nWe are going to answer this question with our plot: How does the average yearly change in temperature vary within a given country?\nTo do this, we will write a function called temperature_coefficient_plot that takes 6 parameters: db_file, country, year_begin, year_end, month, min_obs, **kwargs - db_file is the name of our database file. In this case, its “temps.db” - country is the name of the country we are inquiring about. - year_begin is the beginning of the range of years we are inquiring about - year_end is the ending of the range of years we are inquiring about - month is the month number we are inquring about (example: January is 1) - min_obs is the minimum number of observations a station needs to have reported to be included in our graph\nTo measure the average yearly change, we will have to use linear regression to create a model and find the coefficient representing the estimated yearly increase. We will then have to put this into a data frame, find each stations latitude and longitude, and plot it geographically.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \n    def estimate_yearly_increase(df):\n        \"\"\"\n    Estimates the yearly increase in temperature based on a linear regression model.\n\n    Parameters:\n    - df: inputted dataframae containing temperature data with columns 'YEAR' and 'TEMP'.\n\n    Returns:\n    estimated_yearly_increase: Estimated yearly increase in temperature. This is the coefficient of a linear regression model.\n    The linear regression model is fitted using the 'YEAR' column as the independent variable and the 'TEMP'\n    column as the dependent variable.\n    \"\"\"\n        \n        \n        \n        X = df[[\"YEAR\"]]\n        y = df[\"TEMP\"]\n\n        # Create a linear regression model\n        model = LinearRegression()\n\n        # Fit the model\n        model.fit(X, y)\n\n        # Get the coefficient representing the estimated yearly increase\n        estimated_yearly_increase = model.coef_[0]\n\n        return estimated_yearly_increase\n\n    #uses above funciton to create data frame\n    test = query_climate_database(db_file, country, year_begin, year_end, month)\n\n    #groups all the stations w same lattide and longitude together, then applies the regression\n    result = test.groupby(['LATITUDE', 'LONGITUDE']).apply(estimate_yearly_increase).reset_index()\n\n    #this gives us how many rows relate to each station\n    groups = test.groupby(['LATITUDE', 'LONGITUDE']).transform('size')\n\n    #adds a size column\n    result[\"size\"] = groups\n\n    # filter out rows with a size less than min_obs\n    result = result[result['size'] &gt;= min_obs]\n\n    # merge the original data frame with the one that underwent linear regression\n    merged = pd.merge(result, test, on=['LATITUDE', 'LONGITUDE'])\n\n    \n    # rename columns\n    merged.columns = [\"LATITUDE\", \"LONGITUDE\", \"Estimated Yearly Increase (C°)\", \"SIZE\",\n                      \"NAME\", \"COUNTRY\", \"YEAR\", \"MONTH\", \"TEMP\"]\n\n    # round estimated increase\n    merged['Estimated Yearly Increase (C°)'] = merged['Estimated Yearly Increase (C°)'].round(4)\n\n    # create a scatter mapbox plot using plotly \n    fig = px.scatter_mapbox(merged,\n                            title='Estimated Yearly Increase in Temperature',\n                            lat='LATITUDE',\n                            lon='LONGITUDE',\n                            color='Estimated Yearly Increase (C°)',\n                            hover_name='NAME',\n                            mapbox_style=\"carto-positron\")\n\n    return fig\n\nNow test our function\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\nfig = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig\n\n\n\n\nNow we have a geographic graph of India that shows the estiamted yearly increase in temperature for each station. This pulls data from 1980 to 2020 in the month of January\nLets do this one more time for Mexico to ensure our code works correctly. This time we will set the years to be between 1999 and 2020 and for the month of February.\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\nfig2 = temperature_coefficient_plot(\"temps.db\", \"Mexico\", 1999, 2020, 2, \n                                   min_obs = 2,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig2\n#fig.show()\n\n\n\n\n\n\nStep 6:\nNow we will make a function to plot a box plot of temperature throughout the entire country for a given month and year.\nOur function will be called temp_for_year and take 4 arguments - db is the name of our database file. In this case, its “temps.db” - country is the name of the country we are inquiring about. - month is the month number we are inquring about (example: January is 1)\nIt will use the query_climate_database to pull data for the specific country, for the given year, and for the given month. This isn’t a range of years, rather data from a specific year.\nFor our boxplot, we need an x axis, which would be the month The y axis is the temp\n\n# box plot of temperature throughout entire country for a given month and year\n\ndef temp_for_year(db, country, year, month):\n    \"\"\"\n    Generate a box plot visualizing temperature distribution in a specific country for a given year and month.\n\n    Parameters:\n    - db: SQLite database file containing climate data.\n    - country: name of the country for which temperature data is visualized.\n    - year: year for which temperature data is visualized.\n    - month: month for which temperature data is visualized.\n\n    Returns:\n    fig: box plot displaying the distribution of temperatures for the specified country,\n                                  year, and month. The plot is created using Plotly Express.\n\n    \"\"\"\n    \n    \n    table = query_climate_database(db_file = db,\n                       country = country, \n                       year_begin = year,\n                       year_end = year,\n                       month = month)\n\n\n\n    fig = px.box(table,\n             title = f\"Temperature in {country} for {year}\",\n             x = \"MONTH\",\n             y = \"TEMP\",\n             color = \"MONTH\",\n             width = 600,\n             height = 600)\n\n    return fig\n\nNow lets test our function\n\ntemp_for_year(\"temps.db\", \"India\", 2020, 5)\n\n\n\n\nThis shows us that the median temperature in India for May of 2020 was 31.09 degreees celcius.\n\n\nStep 7:\nCreate a graph to visualize the average temperature in the entire country throughout the year, and to compare it to another country.\nTo do this, we will define a function called avg_temp_per_month with 4 parameters: file, country1, country2, year\n\nfile is the name of our database file. In this case, its “temps.db”\ncountry1 is the name of the first country we are inquiring about\ncountry2 is the name of the second country we are inquiring about\nyear is the year we are inquring about\n\ncomments throughout the function definition explain the code\nWe will also import a new query function, called query_climate_database2. This takes 3 parameters: file, country, and year. The function outputs a data frame with the average temperatues across the given country for each calander month of the year.\nFirst, we will import query_climate_database2 and test it. Then we will define the function avg_temp_per_month that makes calls to the query_climate_database2 function.\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database2(db_file, country, year):\n    \"\"\"\n    A new query a climate database to retrieve temperature data for a specific country, year range, and month.\n\n    Parameters:\n    - db_file: database file.\n    - country: name of the country for which climate data is requested.\n    - year: yearfor which temperature data is requested.\n\n    Returns:\n    pd: dataframe containing the monthly average temperature for the specified country and year.\n                  Columns include 'Month', 'ID', 'Average_Temp', and 'Country'.\n                  \n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    cursor = conn.cursor()\n    cursor.execute(f'SELECT C.Name, SUBSTR(T.ID,1,2) FROM temperatures T JOIN countries C WHERE C.Name=\"{country}\" AND SUBSTR(T.ID,1,2) = C.\"FIPS 10-4\"')  \n\n    result = [cursor.fetchone() for i in range(1)]  # get just the first  result\n    #this gives us Name and first two letters\n    \n    letters = result[0][1] # this gives us IN\n    \n    x = cursor.execute(f'SELECT T.Year, T.Temp, T.ID, T.Month FROM temperatures T WHERE SUBSTR(T.ID,1,2)=\"{letters}\" AND T.Year == {year}')\n    # we now have year and temp for the given inputs\n    #still need name of weatherpeerson and latitude/longitude\n\n    cursor = conn.cursor()\n    y=cursor.execute(f'SELECT S.ID FROM stations S LEFT JOIN temperatures T on T.ID = S.ID WHERE SUBSTR(S.ID,1,2)=\"{letters}\"')\n    \n    \n    \n\n    # putting all info into data frame\n    df = pd.DataFrame(x, columns = [\"YEAR\", \"TEMP\", \"ID\", \"Month\"])\n    df2 = pd.DataFrame(y, columns = [\"ID\"])\n    df2[\"COUNTRY\"] = country\n    #print(cursor.fetchall())\n    merged = pd.merge(df, df2, on='ID')\n    dropped = merged.dropna()\n    final = dropped.drop_duplicates()\n    \n    \n    mean_temps = final.groupby(['ID', 'Month', 'COUNTRY'])['TEMP'].mean()\n\n    # reset index to turn the result into a DataFrame\n    mean_temps_df = mean_temps.reset_index()\n\n    # group by 'Month' and calculate the mean temperature for each month\n    monthly_mean_temps = mean_temps_df.groupby('Month')['TEMP'].mean()\n\n    # creates a new DataFrame with 'Month', 'ID', and 'Average_Temp' columns\n    result_df = pd.DataFrame(monthly_mean_temps.reset_index())\n    result_df['Country'] = country\n    \n    #rounds the temps temps to 4 decimal places\n    result_df['TEMP'] = result_df['TEMP'].round(4)\n\n    # adds each months name in the month column\n    result_df['Month'] = [\"Jan\", \"Feb\", \"Mar\", \"April\", \"May\", \"June\", \"July\", \"Aug\", 'Sept', \"Oct\", \"Nov\", \"Dec\"]\n\n\n    #return the result!\n    return result_df\n\n\n\n\n# testing our new query function - it will give us the average temperature across India for each month in 1980\ntest = query_climate_database2(\"temps.db\", \"India\", 1980)\ntest\n\n\n\n\n\n\n\n\nMonth\nTEMP\nCountry\n\n\n\n\n0\nJan\n19.9226\nIndia\n\n\n1\nFeb\n22.5644\nIndia\n\n\n2\nMar\n25.4641\nIndia\n\n\n3\nApril\n29.1662\nIndia\n\n\n4\nMay\n30.7662\nIndia\n\n\n5\nJune\n28.4487\nIndia\n\n\n6\nJuly\n27.0070\nIndia\n\n\n7\nAug\n26.7079\nIndia\n\n\n8\nSept\n27.1199\nIndia\n\n\n9\nOct\n26.2906\nIndia\n\n\n10\nNov\n23.1583\nIndia\n\n\n11\nDec\n20.2473\nIndia\n\n\n\n\n\n\n\nAs we can see, the average temperature for each month in India for 1980 was calcuated.\nNow its time to write our new graphing function\n\n# now we can add our new graphing function\ndef avg_temp_per_month(file, country1, country2, year):\n    \"\"\"\n    Generate a scatter plot comparing the average temperature in each month for two countries in a specific year.\n\n    Parameters:\n    - file:  SQLite database file containing climate data.\n    - country1: Name of the first country for temperature data comparison.\n    - country2: Name of the second country for temperature data comparison.\n    - year: Year for which average temperature data is compared.\n\n    Returns:\n    fig: Scatter plot displaying the average temperature in each month for two countries.\n                                 The plot is created using Plotly Express (px.scatter).\n    \"\"\"\n    \n    \n    \n    \n    #uses query function for country 1\n    c1 = query_climate_database2(db_file = file,\n                       country = country1, \n                       year= year)\n    \n    \n    # uses query function for country 2\n    c2 = query_climate_database2(db_file = file,\n                       country = country2, \n                       year= year)\n    \n    \n    # places the data frame of country 1 ontop of the data frame of country 2 to make one long data frame\n    stacked = pd.concat([c1, c2], ignore_index=True)\n\n    \n    # graphs the data frame\n    fig = px.scatter(data_frame = stacked,\n                 title=f\"Average Temp in each month for {country1} vs {country2} \",\n                 x = \"Month\",\n                 y = \"TEMP\",\n                 color = \"TEMP\",\n                 hover_name = \"TEMP\",\n                 hover_data = [\"Month\", \"TEMP\"],\n                 #size = \"Body Mass (g)\",\n                 #size_max = 8,\n                 width = 800,\n                 height = 500,\n                 #opacity = 0.5\n                 facet_col = \"Country\"\n                )\n\n    \n    return fig\n\n\n# now we can use the avg\navg_temp_per_month(\"temps.db\",\"India\",\"China\", 2020)\n\n\n\n\nAs we can see, the graphs display a side by side comparison of the average temps of 2 countries per month."
  },
  {
    "objectID": "posts/hw 3/Untitled.html",
    "href": "posts/hw 3/Untitled.html",
    "title": "HW 3 A Fun Message Bank!",
    "section": "",
    "text": "We are going to create a simple message bank using flask! This message bank will let users submit a message with their name, and view other’s messages!\n\nStep 1: Install Packages\n\nfrom flask import Flask, render_template, request, g\nfrom flask import redirect, url_for, abort\nimport sqlite3\n\napp = Flask(__name__)\n\nWe will be using flask to build our webpage. Additionally, we will use sqlite3 to create a database to house the messages. Finally, we will name our app.\n\n\nStep 2: Create a database function to house the messages\n\ndef get_message_db():\n    try:\n     # attempt to return the existing message database connection from the global context (g)\n     # checks to see if the message_db exists, if it doesnt't it will create one using the except:\n\n        return g.message_db\n    except: \n        # creates the database\n        g.message_db = sqlite3.connect(\"message_db.sqlite\")\n        \n        # creates a table messages inside the data base with a handle and message column\n        # IF NOT EXISTS means this command will only be ran if the messages table doesnt exist\n        cmd = 'CREATE TABLE IF NOT EXISTS messages (handle TEXT, message TEXT)'\n        \n        # connects to database\n        cursor = g.message_db.cursor()\n        cursor.execute(cmd)\n        return g.message_db\n\nWe are defining the get_message_db function. This function will check to see if a database already exists, and if it doesn’t, we will create one. This is the try and except part. If the databse exists, it will just return the database. If it doesn’t, we will create the databse using sqlite. Addtionally, we will use the command CREATE TABLE IF NOT EXISTS to add a messages table to the database. This table will have a handle column and a message column. The handle will be used for the names of the messengers, and the message column will be for the messages from the users.\n\n\nStep 3: Create a database function to house the messages\n\ndef insert_message(request):\n    # Extract the message and handle from the request\n    message = request.form.get(\"message\")\n    name = request.form.get(\"name\")\n    \n    \n    # gets database\n    database = get_message_db()\n    \n    # connects to database\n    conn = sqlite3.connect('message_db.sqlite')\n    \n    \n    cursor = conn.cursor()\n   \n    # inserts the message and handle into the messages table\n\n       # creates a function to insert the values into the table\n        # the ? represents where the paremeters of name, messages will be passed too\n    insert = \"\"\"\n    INSERT INTO messages (handle, message) VALUES (?, ?);\n    \"\"\"\n    cursor.execute(insert, (name, message))\n    \n     # commits the changes and close the connection\n    conn.commit()\n    conn.close()\n        \n        # returns the message and name that was submitted\n    return message, name\n\n\n\nStep 1:\n\n\nStep 1:\n\n\nStep 1:\n\n\nStep 1:\n\n\nStep 1:"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/hw2/index.html",
    "href": "posts/hw2/index.html",
    "title": "HW 2 Scrapy Movies Recomendations",
    "section": "",
    "text": "This code is a web crawling and scraping framework in Python. It uses a spider to scrape. The spider navigates through movie pages on themoviedb.org, extracts information about actors, and retrieves details about their movies or TV shows.\n\nStep 1: Import packages\n\nimport scrapy\n\n\n\nStep 2: Writing the Spider Class:\n\nclass TmdbSpider(scrapy.Spider):\n    \n    # name of spider\n    name = 'tmdb_spider'\n\nWe create the TmdbSpider class usign scrapy.Spider, and name our webscraper ‘tmdb_spider’\n\n\nStep 3: Constructor and Initialization:\n\n# constructor - starts spider with the starting URL\ndef __init__(self, subdir=None, *args, **kwargs):\n    self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nThe constructor starts the spider with a starting URL based on the provided subdir. The subdir is the back half of the link for the specific movie we are inquring about.\n\n\nStep 4: First parsing method\n\n# assuming we are starting on the movie page, and then navigating to the Full Cast & Crew Page\ndef parse(self, response):\n    \"\"\"\n    Parses TMDB movie pages.\n    Assumes that you start on a movie page, and then navigates to the Full Cast & Crew page.\n    Yields a scrapy.Request for the \"Full Cast & Crew\" page, with the parse_full_credit method specified in the callback argument.\n    Does not return any data.\n    \"\"\"\n    # full cast and crew page has url: &lt;movie_url&gt;cast\n    full_credits_url = response.url + \"/cast/\"\n      \n    # yielding a scrapy request using full_credits_url and callback\n    # callback means calling parse_full_credits(self, responce) using the full_credits_url ( so we have navigated -\n    # to the full cast and crew page\n    yield scrapy.Request(full_credits_url, callback=self.parse_full_credits)\n\nIn the parse method, the spider navigates from the movie page to the Full Cast & Crew page by adding “/cast/” to the current URL. A scrapy request is yielded with the URL of the Full Cast & Crew page and the callback function parse_full_credits.\n\n\nStep 5: Second Parsing Method\n\n    # parse_full_credits to get the actor URLs from the Full Cast & Crew page\ndef parse_full_credits(self, response):\n     \"\"\"\n     Parses TMDB \"Full Cast & Crew\" pages.\n     Assumes that you start on the Full Cast & Crew page\n     Yields a scrapy.Request for the page of each cast member, with the parse_actor_page method specified in the callback argument.\n     Does not return any data.\n     \"\"\"\n   \n    # this gets each actor URL from the full cast and crew page using CSS selector   \n    #gets full info for all actors\n     actors = response.css('div.info')\n        \n     # extracts indidual urls of each actor\n     actors = actors.css('div.info &gt; p &gt; a::attr(href)').getall()\n        \n\n       # iterates thru each actor URL and yields a request for the actor url and the parse_actor_page as the callback\n     for urlx in actors:\n        #yield {\"actor\" :\"https://www.themoviedb.org\" + urlx}\n        yield scrapy.Request(\"https://www.themoviedb.org\" + urlx, callback=self.parse_actor_page)\n\nThe parse_full_credits method extracts actor URLs from the Full Cast & Crew page using CSS selectors. For each actor URL, a scrapy request is yielded with the actor URL and the callback function parse_actor_page.\n\n\nStep 6: Final Parsing Method\n\n# parse_actor_page to get actor name and their movies/tv shows from the actor's page\ndef parse_actor_page(self, response):\n    \"\"\"\n    Parses TMDB actor pages.\n    Assumes we are on each actor's individual page\n    Yields a dictionary containing the actor's name(\"actor\") and the movie/TV (\"movie_or_TV_name\") for each movie/TV show they have acted in.\n    This dictionary does not include any movie/TV they were a cast in.\n    \"\"\"\n     \n    # get the actor's name from the actor's page\n    name = response.css('div.title &gt; h2 &gt; a::text').get()\n\n\n        \n    # looks for acting header, saves it to index\n    index = response.css('div.credits_list h3::text').getall().index(\"Acting\")\n        \n    # looks for acting roles (exluding all other roles), so it uses the index of the acting header\n    roles = response.css('div.credits_list table.card.credits')[index]\n        \n    # gets name of the roles\n    final = roles.css('a.tooltip bdi::text').getall()\n        \n    # iterates through each move/tv show and gets a dictionary with actor and movie/TV show names\n    for movie_or_TV_name in final:\n            yield {\"actor\": name, \"movie_or_TV_name\": movie_or_TV_name}\n                \n\nThe parse_actor_page method extracts the actor’s name from the actor’s page. It finds the index of the “Acting” header and extracts the roles (movies/TV shows). It only takes the movies/TV shows the actor starred in, and leaves out any set they worked on as a crew or production.\nA dictionary with actor name and movie/TV show details is yielded for each entry. This dictionary displays all the movies/tv shows each actor in the movie additionally worked on\nThe spider, when run with the command scrapy crawl tmdb_spider -o movies.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone, will start from the specified movie page on themoviedb.org, navigate to the Full Cast & Crew page, and collect information about actors and their associated movies or TV shows. The output will be saved to a CSV file named movies.csv.\n\n\nStep 7: Analyzing Data\nNow that the date is in a movies.csv file, we can read the data using pandas data frame\n\n# reading in our data\nimport pandas as pd\ndf = pd.read_csv(\"results.csv\")\n\n\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nDaniel Radcliffe\nHave I Got a Bit More News for You\n\n\n1\nDaniel Radcliffe\nDavid Holmes: The Boy Who Lived\n\n\n2\nDaniel Radcliffe\n100 Years of Warner Bros.\n\n\n3\nDaniel Radcliffe\nMulligan\n\n\n4\nDaniel Radcliffe\nDigman!\n\n\n...\n...\n...\n\n\n2952\nRupert Grint\nThe View\n\n\n2953\nRupert Grint\nGMTV\n\n\n2954\nRupert Grint\nThe Tonight Show with Jay Leno\n\n\n2955\nRupert Grint\nAn Audience with...\n\n\n2956\nRupert Grint\nToday\n\n\n\n\n2957 rows × 2 columns\n\n\n\nNow we can see our data frame has a column for each actor, and another column with the movie or tv show name that they’ve starred in. We can clean up this data frame a bit.\n\n# need column of movies and a column of number of shared actors for that movie\n\n# value_counts() counts the number of times each movie or tv name appears in the column\nnumber = df[\"movie_or_TV_name\"].value_counts()\n# we can see now that Harry Potter and the Philosopher's Stone appears 63 times, \n# and Harry Potter and the Chamber of Secrets appears 37 times, etc.\n\n# puts into a new dataframe\nfinal = number.to_frame()\nfinal = final.reset_index()\n\n# renames column\nfinal.rename(columns={\"count\": \"Number of shared actors\"},inplace=True)\n\n# removing the first row since that is Harry Potter and the Philosopher's Stone, which is the movie we inquired about\nfinal = final.iloc[1:]\n\n# now this shows us which TV shows or Movies have the most number of shared actors to \n# Harry Potter and the Philosopher's Stone  \nfinal\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nNumber of shared actors\n\n\n\n\n1\nHarry Potter and the Chamber of Secrets\n37\n\n\n2\nCreating the World of Harry Potter\n36\n\n\n3\nHarry Potter and the Prisoner of Azkaban\n26\n\n\n4\nHarry Potter and the Order of the Phoenix\n24\n\n\n5\nHarry Potter and the Deathly Hallows: Part 2\n23\n\n\n...\n...\n...\n\n\n2270\nHetty Wainthropp Investigates\n1\n\n\n2271\nIntimate Relations\n1\n\n\n2272\nBrazen Hussies\n1\n\n\n2273\nBathtime\n1\n\n\n2274\nToday\n1\n\n\n\n\n2274 rows × 2 columns\n\n\n\nNow we can iterate through our data and take only the movies that have 7 common actors. This would then be our best movie recomendations\n\n# iterates through data frame and removes all Tv Shows/Movies with less than 7 common actors\nfor index, row in final.iterrows():\n    if row['Number of shared actors'] &lt; 7:\n        final.drop(index, inplace=True)\n\nFinally, we can plot our movie recomendations\n\n# using plotly to graph the data:\n\n#importing packages\nimport plotly.io as pio\nfrom plotly import express as px\npio.renderers.default=\"iframe\" \n\n\n# graphing\n# x axis is movie_or_Tv name\n# y axis is Number of shared actors\nfig = px.bar(final,\n                x=\"movie_or_TV_name\",\n                y=\"Number of shared actors\",\n                title = \"Movies/TV Shows with at least 7 actors in common\")\nfig.show()\n\n\n\n\nIt looks like the Harry Potter Franchise is a big recommendation, along with Doctor Who and Performance"
  },
  {
    "objectID": "posts/bruin/bruin.html",
    "href": "posts/bruin/bruin.html",
    "title": "Bruin",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/posts/index.html",
    "href": "posts/posts/index.html",
    "title": "HW 0",
    "section": "",
    "text": "Import all necessary packages, and import the Palmer Penguins data set. For this assignment, we use the pandas package and the matplotlib.pyplot package.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\ndf = pd.read_csv(url)"
  },
  {
    "objectID": "posts/posts/index.html#step-1-importing",
    "href": "posts/posts/index.html#step-1-importing",
    "title": "HW 0",
    "section": "",
    "text": "Import all necessary packages, and import the Palmer Penguins data set. For this assignment, we use the pandas package and the matplotlib.pyplot package.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\ndf = pd.read_csv(url)"
  },
  {
    "objectID": "posts/posts/index.html#step-2-previewing-data",
    "href": "posts/posts/index.html#step-2-previewing-data",
    "title": "HW 0",
    "section": "Step 2: Previewing Data",
    "text": "Step 2: Previewing Data\nPreview the data frame to see the column names and better understand what we’re working with.\n\ndf\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\nPAL0910\n120\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A2\nNo\n12/1/09\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n340\nPAL0910\n121\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN39A1\nYes\n11/22/09\n46.8\n14.3\n215.0\n4850.0\nFEMALE\n8.41151\n-26.13832\nNaN\n\n\n341\nPAL0910\n122\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN39A2\nYes\n11/22/09\n50.4\n15.7\n222.0\n5750.0\nMALE\n8.30166\n-26.04117\nNaN\n\n\n342\nPAL0910\n123\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN43A1\nYes\n11/22/09\n45.2\n14.8\n212.0\n5200.0\nFEMALE\n8.24246\n-26.11969\nNaN\n\n\n343\nPAL0910\n124\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN43A2\nYes\n11/22/09\n49.9\n16.1\n213.0\n5400.0\nMALE\n8.36390\n-26.15531\nNaN\n\n\n\n\n344 rows × 17 columns"
  },
  {
    "objectID": "posts/posts/index.html#step-3-breaking-up-the-data",
    "href": "posts/posts/index.html#step-3-breaking-up-the-data",
    "title": "HW 0",
    "section": "Step 3: Breaking up the data",
    "text": "Step 3: Breaking up the data\nWe see in the data there are three types of penguins: Adelie, Gentoo, and Chinstrap. We also see there’s data on each penguins flipper length and body mass.\nWe can build a scatterplot to see the relationship between the penguin’s flipper lenght and body mass. We can take this one step further and split this scatterplot up into the three types of penguins.\nLet’s start by breaking up the data frame into three smaller data frames. Each data frame will take all the entries pretaining to an individual species of penguin. Therefore, we will have 3 new data frames (since there are three species of penguins).\n\n# breaking up the data to be with certain species\nadelie = df[df[\"Species\"] == \"Adelie Penguin (Pygoscelis adeliae)\"]\ngentoo = df[df[\"Species\"] == \"Gentoo penguin (Pygoscelis papua)\"]\nchinstrap = df[df[\"Species\"] == \"Chinstrap penguin (Pygoscelis antarctica)\"]\n\nNow, we can preview the new data frame pretaining to the Adelie species.\n\nadelie\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n147\nPAL0910\n148\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN83A2\nYes\n11/13/09\n36.6\n18.4\n184.0\n3475.0\nFEMALE\n8.68744\n-25.83060\nNaN\n\n\n148\nPAL0910\n149\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN84A1\nYes\n11/17/09\n36.0\n17.8\n195.0\n3450.0\nFEMALE\n8.94332\n-25.79189\nNaN\n\n\n149\nPAL0910\n150\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN84A2\nYes\n11/17/09\n37.8\n18.1\n193.0\n3750.0\nMALE\n8.97533\n-26.03495\nNaN\n\n\n150\nPAL0910\n151\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN85A1\nYes\n11/17/09\n36.0\n17.1\n187.0\n3700.0\nFEMALE\n8.93465\n-26.07081\nNaN\n\n\n151\nPAL0910\n152\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN85A2\nYes\n11/17/09\n41.5\n18.5\n201.0\n4000.0\nMALE\n8.89640\n-26.06967\nNaN\n\n\n\n\n152 rows × 17 columns"
  },
  {
    "objectID": "posts/posts/index.html#step-4-building-the-scatterplot",
    "href": "posts/posts/index.html#step-4-building-the-scatterplot",
    "title": "HW 0",
    "section": "Step 4: Building the Scatterplot",
    "text": "Step 4: Building the Scatterplot\nNow that we have our individual data frames, we can build our scatterplot using the plt.scatter command. We will index the data within the columns “Body Mass (g)” and “Flipper Length (mm)” for our x and y axis. We will color code each speciesof penguin as well.\nWe can also add x and y axis labels, a plot title, and a legend.\n\nplt.scatter(adelie['Body Mass (g)'], adelie['Flipper Length (mm)'], marker='o', linestyle='', color='b', label='Adelie Penguin')\nplt.scatter(gentoo['Body Mass (g)'], gentoo['Flipper Length (mm)'], marker='o', linestyle='', color='red', label='Gentoo Penguin')\nplt.scatter(chinstrap['Body Mass (g)'], chinstrap['Flipper Length (mm)'], marker='o', linestyle='', color='green', label='Chinstrap Penguin')\n\n\nplt.xlabel('Body Mass (grams)')\nplt.ylabel('Flipper Length (mm)')\nplt.title('Plotting Flipper Length vs Body Mass')\n\n\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\nModuleNotFoundError: No module named 'climate_database'"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome! This is Nico TestingW!!!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog2",
    "section": "",
    "text": "HW 3 A Fun Message Bank!\n\n\n\n\n\n\nweek 5\n\n\nHomework\n\n\n\n\n\n\n\n\n\nFeb 10, 2024\n\n\nNico Morrone\n\n\n\n\n\n\n\n\n\n\n\n\nHW 2 Scrapy Movies Recomendations\n\n\n\n\n\n\nweek 4\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nNico Morrone\n\n\n\n\n\n\n\n\n\n\n\n\nHW 1 - Analyzing and Comparing Temps of Various Countries\n\n\n\n\n\n\nweek 3\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nNico Morrone\n\n\n\n\n\n\n\n\n\n\n\n\nHW 0\n\n\n\n\n\n\nweek 0\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nNico Morrone\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\nBruin\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\nNico\n\n\n\n\n\n\nNo matching items"
  }
]